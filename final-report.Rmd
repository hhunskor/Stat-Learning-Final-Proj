---
title: "Math 218: Final Report"
subtitle: "Project Haydoja"
author: "Payoja and Hayden"
output: pdf_document
date: "?"
editor_options: 
  chunk_output_type: inline
---

```{r packages-data, include = F}
library('fastDummies')
library(tidyverse)
library(dplyr)
library(tidyr)
library(ggcorrplot)
dat <- read_csv("data/train.csv")
summary(dat)
lapply(dat,class)

```


## Introduction

For our final project, we decided to look at the Ames Housing dataset, which includes extensive information on the sale of residential properties in Ames, Iowa from 2006 to 2010. It contains 1460 observations with 79 variables on various house specifications. The variables are a mix of continuous, categorical and discrete types. The data is publicly available on Kaggle - below is a link to the dataset itself which includes a description of all the data fields:

https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data

The goal of our project is to implement as many of the modeling techniques we've covered in this course as possible. To do so, we decided to focus on two modeling problems: one regression and one classification.

Below are our research questions:

1. Which modeling technique most accurately predicts housing prices using the other available housing features?

2. Which modeling technique most accurately classifies houses by neighborhood based on the other available housing features?


### Data Description

Rather than use all 79 variables from the dataset for our analysis, we decided to select a few that we found particularly interesting. The variables that we have decided to use in this report is listed below.

##### Categorical Variable:


- Neighborhood: Physical locations within Ames city limits

- CentralAir: Central air conditioning (Yes/No)

- PavedDrive: Paved driveway

- GarageFinish: Interior finish of the garage

##### Continuous Variable:

- SalePrice: Price of house 

- PoolArea: Pool area in square feet

- Lot Area: Lot size in square feet

- TotalBsmtSF: Total square feet of basement area

- Garage Area :Size of garage in square feet

- GrLivArea: Above ground living area square feet

##### Discrete Variable

- FullBath: Number of full bathrooms above ground

- HalfBath: Number of half bathrooms above ground

- BedroomAbvGr: Number of bedrooms above ground

- KitchensAbvGr: Number of kitchens above ground

- FirePlaces: Number of fireplaces

- YrBuilt: Original construction date

- YrSold: Year house was sold





list out all 79 variables from our dataset, we selected a few that we found particularly interesting and listed them below. That said, we are interested in starting out with models that include all available predictors and applying various shrinkage techniques.



## EDA


#### Summary Statistics of Numeric Variables
```{r echo = F}
# load data
library(tidyverse)
dat <- read_csv("data/train.csv")
#Find numeric variables
numeric_var <- names(dat)[which(sapply(dat, is.numeric))]

#Find categorical variables
categorical_var <- names(dat)[-which(sapply(dat, is.numeric))]

#Numeric variables that we want to work with
var <- numeric_var[-c(1:3,5:6,8:12,14:16,18:19,24, 26:27,29:33,35:36)]

#Summary table for numeric variables
dat[var] %>%
  summarise_all(list(mean = mean, std = sd), na.rm = TRUE) %>%
  pivot_longer(cols = everything(), 
               names_to = c('col', '.value'), 
               names_sep = '_')
```

#### Closer Look at Sale Price

The following code chunk takes a deeper look at the SalePrice feature:

```{r}
#Histogram of house prices
dat %>%
  ggplot(., aes(x = SalePrice)) +
  geom_histogram()

#Average house price over time
dat %>%
  group_by(YrSold) %>%
  summarise(meanPrice = mean(SalePrice)) %>%
  ggplot(., aes(x = YrSold, y = meanPrice)) +
  geom_point() + geom_line()
```

Judging from these two plots, the distribution of SalePrice in our training set is slightly right skewed. This suggests that there are some outlier properties with significantly higher sale prices.

Also, the huge drop in average sale price of houses in our data coincides with the 2008 housing market crash. When predicting sale price it may be helpful to try to account for some of the macroeconomic factors at play, such as the general state of the housing market at a national level. This may be beyond the scope of our work, but definitely something worth addressing in our project limitations.


#### Correlation between some of the numeric and discrete data fields (most importantly SalePrice):

```{r}

#Note that we are only using the training data here because it has actual values for SalePrice

correlation <- dat %>%
  select(SalePrice, PoolArea, LotArea, TotalBsmtSF, GarageArea, GrLivArea, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, Fireplaces,YrSold, YearBuilt)

corr_mtx <- cor(correlation, use = "pairwise.complete.obs")

ggcorrplot(corr_mtx, # input the correlation matrix
           hc.order = TRUE, 
           type = "upper", 
           lab = TRUE,
           method = "circle", 
           colors = c("tomato2", "white", "springgreen3"))
```
As seen from above, housing prices seem to be the most correlated (positively) with above ground living area (in sq. ft), total basement area(in sq. ft) and garage area (in sq. ft). The price is also moderately correlated with number of full bathrooms and year built.



### Data Cleaning

As said earlier, of the 79 available variables, we will only be working with 17 variables--4 categorical, 6 continuous and 7 numeric--in this report. Thus, we will drop all other variables from out dataset.

```{r}
#data with numerical variables of our choice
numeric_dat<-dat[var]
#data with numerical variables of our choice
cat_var<-categorical_var[c(9,29,35,38)]
categorical_dat<-dat[cat_var]
#final data with the 17 variables variables of our choice
final_dat <- cbind(numeric_dat,categorical_dat)
```

In the dataset from Kaggle, PavedDrive is a categorical varible with three possible class: Y (if paved), P (if partially paved) and N(if dirt/gravel). We recode the variable so that PavedDrive is a binary variable that takes Y(Yes) if it is paved and N (No) otherwise.
```{r}
final_dat$PavedDrive <- ifelse((final_dat$PavedDrive=="Y"), "Y", "N")
```


## Methodology


### Ridge Regression







## Results

###Linear Regression
```{r}
#create dummy variable for PavedDrive
#final_dat$PavedDrive <- ifelse((final_dat$PavedDrive=="Y"), 1, 0)
#create dummy variable for CentralAir
#final_dat$CentralAir <- ifelse((final_dat$CentralAir=="Y"), 1, 0)
# create dummy variables for all categorical variables
dataf <- dummy_cols(final_dat,remove_first_dummy = TRUE)
#drop the columns that stores categorical variable since thee same information 
# is stored in the newly created dummy variables
cat_name<-names(final_dat)[-which(sapply(final_dat, is.numeric))]
dataf <-dataf[,!(names(dataf) %in% cat_name)]
# for numeric variaables, the dataset has NA for 0. so replace NA with 0s.
dataf[is.na(dataf)] = 0
```

```{r}
set.seed(1)
# create vector of indices for each set
train_ids <- sample(1:n, .80*n)
test_ids <- (1:n)[-train_ids]
# create test and train set based on the indicators
train_set<-dataf[train_ids,]
test_set<-dataf[test_ids,]
model<-lm(SalePrice ~ ., train_set)
preds <- predict(model, test_set)
sqrt(mean((test_set$SalePrice-preds)^2))
coef(model)
```


### Ridge Regression


```{r}
library(glmnet)
x <- model.matrix(SalePrice ~ ., dataf)[,-1]
y<-dataf$SalePrice
n <- nrow(dataf)


```

```{r}
set.seed(10)
y_test<-y[test_ids]
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 0, nfolds = 10)
best_lam <- cv_out$lambda.min
best_lam
#find RMSE
grid <- 10^seq(5,-2,length = 100)
ridge_mod <- glmnet(x[train_ids,], y[train_ids], alpha = 0, lambda = grid)
ridge_preds <- predict(ridge_mod, s= best_lam, newx = x[test_ids,])
sqrt(mean((ridge_preds - y_test)^2))
ridge_final <- glmnet(x, y, alpha = 0, lambda = best_lam)
round(coef(ridge_final), 3)
```

### Lasso Regression

```{r}

cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 1, nfolds = 10)
best_lam <- cv_out$lambda.min
best_lam
grid <- 10^seq(5,-2,length = 100)

#coefficients
lasso_mod <- glmnet(x[train_ids,], y[train_ids],alpha = 1, lambda = grid)
lasso_coef <- predict(lasso_mod, type = "coefficients", s = best_lam)[1:18,]
lasso_coef
#RMSE
sqrt(mean((predict(lasso_mod, s= best_lam, newx = x[test_ids,]) - y_test)^2))
```

#Naive Bayes

```{r}
library(e1071)
final_dat$GarageFinish[is.na(final_dat$GarageFinish)] <- "NoGrg"
train_set<-final_dat[train_ids,]
test_set<-final_dat[test_ids,]
# Fitting Naive Bayes Model
# to training dataset
set.seed(120)  # Setting Seed
classifier_cl <- naiveBayes(Neighborhood ~ .-YearBuilt,train_set)
#classifier_cl
 
# Predicting on test data'
y_pred <- predict(classifier_cl, newdata = test_set)
 
# Confusion Matrix
cm <- table(test_set$Neighborhood, y_pred)
 
# Model Evaluation
mean(test_set$Neighborhood != y_pred)
```

#KNN
```{r}
library(class)
knn_dat<-final_dat%>%
  mutate(CentralAir=ifelse(final_dat$CentralAir=="Y", 1, 0),
         PavedDrive=ifelse(final_dat$PavedDrive=="Y", 1, 0))
#final_dat$CentralAir <- ifelse(final_dat$CentralAir=="Y", 1, 0)
#final_dat$PavedDrive <- ifelse(final_dat$PavedDrive=="Y", 1, 0)
knn_dat_scale <- knn_dat %>%
  mutate_if(is.numeric, scale)%>%
  select(-GarageFinish)
train_set<-knn_dat_scale[train_ids,]
test_set<-knn_dat_scale[test_ids,]

knn_preds <- knn(train = train_set%>%dplyr::select(-Neighborhood),,
                 test = test_set%>% dplyr::select(-Neighborhood),,
                 cl =train_set$Neighborhood, 
                 k = 10)
#table(pred = knn_preds, truth = test_set$Neighborhood)
mean(test_set$Neighborhood != knn_preds)
```


###Tree
```{r}
#final_dat$GarageFinish[final_dat$GarageFinish == 'NA'] <- 'NoGrg'
final_dat$GarageFinish[is.na(final_dat$GarageFinish)] <- "NoGrg"
#final_dat$CentralAir <- ifelse(final_dat$CentralAir=="Y", 1, 0)
#final_dat$PavedDrive <- ifelse(final_dat$PavedDrive=="Y", 1, 0)
```


```{r}
library(tree)
final_dat <- final_dat %>%
  mutate_if(is.character, factor)
#final_dat$GarageFinish<-as.factor(final_dat$GarageFinish)
#final_dat$CentralAir<-as.factor(final_dat$CentralAir)
#final_dat$PavedDrive<-as.factor(final_dat$PavedDrive)
#final_dat$Neighborhood<-as.factor(final_dat$Neighborhood)
#final_dat$YearBuilt<-as.factor(final_dat$YearBuilt)

train_set<-final_dat[train_ids,]
test_set<-final_dat[test_ids,]
tree_penguin<-tree(Neighborhood ~ .-GarageFinish,train_set)
summary(tree_penguin)
plot(tree_penguin)
text(tree_penguin,pretty=0,cex=.5)
tree_preds <- predict(tree_penguin, test_set, type = "class")
mean(test_set$Neighborhood != tree_preds)

```
```{r}
dat %>%
 group_by(Neighborhood) %>%
summarise(count = n())%>%
  arrange(count)
```

### XgBoost

```{r}
library(xgboost)
```

```{r}
# Convert the Species factor to an integer class starting at 0
# This is picky, but it's a requirement for XGBoost
neighborhood <- final_dat$Neighborhood
label <- as.integer(final_dat$Neighborhood)-1
final_dat$Neighborhood <- NULL
xgboost_dat<-final_dat%>%
  mutate(CentralAir=ifelse(final_dat$CentralAir=="Y", 1, 0),
         PavedDrive=ifelse(final_dat$PavedDrive=="Y", 1, 0))
#final_dat$CentralAir <- ifelse(final_dat$CentralAir=="Y", 1, 0)
#final_dat$PavedDrive <- ifelse(final_dat$PavedDrive=="Y", 1, 0)
xgboost_dat <-xgboost_dat %>%
  select(-GarageFinish)
train_set = as.matrix(xgboost_dat [train_ids,])
train_label = label[train_ids]
test_set = as.matrix(xgboost_dat [-train_ids,])
test_label = label[-train_ids]
```

```{r}
xgb_train = xgb.DMatrix(data=train_set,label=train_label)
xgb_test = xgb.DMatrix(data=test_set,label=test_label)
```

```{r}
# Define the parameters for multinomial classification
num_class = length(levels(neighborhood))
params = list(
  booster="gbtree",
  eta=0.001,
  max_depth=5,
  gamma=3,
  subsample=0.75,
  colsample_bytree=1,
  objective="multi:softprob",
  eval_metric="mlogloss",
  num_class=num_class
)
```


```{r}
xgb.fit=xgb.train(
  params=params,
  data=xgb_train,
  nrounds=1000,
  early_stopping_rounds=10,
  watchlist=list(val1=xgb_train,val2=xgb_test),
  verbose=0
)
```


```{r}
xgb.pred = predict(xgb.fit,test_set,reshape=T)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(neighborhood)

# Use the predicted label with the highest probability
xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
xgb.pred$label = levels(neighborhood)[test_label+1]

# Calculate the final accuracy
result = sum(xgb.pred$prediction==xgb.pred$label)/nrow(xgb.pred)
```




## Discussion