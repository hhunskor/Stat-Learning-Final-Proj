---
title: "Math 218: Final Report"
subtitle: "Project Haydoja"
author: "Payoja and Hayden"
output: pdf_document
date: "Dec 6"
editor_options: 
  chunk_output_type: inline
---

```{r packages-data, include = F}
library('fastDummies')
library(tidyverse)
library(dplyr)
library(tidyr)
library(ggcorrplot)
dat <- read_csv("data/train.csv")
summary(dat)
lapply(dat,class)

```


## Introduction

There is a plethora of statistical tools available to understand any given set of data. Statisticians often use either supervised or unsupervised statistical learning methods to examine the data and build models to try and perform inference and perform inference and make predictions. For the purpose of our project, however, we decided to focus on supervised learning mainly for two reasons. First, the objective is clearly defined in supervised learning as there is a a response variable guiding the analysis. Second, it is easier to assess the performance of  models, for example by determining missclassification rate (for classification problem) or root mean squared error(for prediction problem), in supervised learning.

To conduct the supervised learning of data, we decided to look at the Ames Housing dataset, which includes extensive information on the sale of residential properties in Ames, Iowa from 2006 to 2010. It contains 1460 observations with 79 variables on various house specifications. The variables are a mix of continuous, categorical and discrete types. The data is publicly available on [Kaggle](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data). 


In our project, we will work with both regression and classification problems and our primary goal is to understand which statistical learning method performs better for the Ames housing dataset. Below are our research questions:

1. Which statistical learning method most accurately predicts housing prices using other available housing features?

2. Which  statistical learning method most accurately classifies houses by neighborhood based on the other available housing features?

Although, our primary goal is to understand which statistical learning method performs better, we will also spend some time exploring and understanding which predictors affect our response variables and how.

### Data Description

Rather than use all 79 variables from the dataset for our analysis, we decided to select 17 variables--4 categorical, 6 continuous and 7 numeric--in this report that we found particularly interesting. The variables that we have decided to use in this report is listed below.

##### Categorical Variable:


- Neighborhood: Physical locations within Ames city limits

- CentralAir: Central air conditioning (Yes/No)

- PavedDrive: Paved driveway (Yes/No)

- GarageFinish: Interior finish of the garage

##### Continuous Variable:

- SalePrice: Price of house 

- PoolArea: Pool area in square feet

- Lot Area: Lot size in square feet

- TotalBsmtSF: Total square feet of basement area

- Garage Area :Size of garage in square feet

- GrLivArea: Above ground living area square feet

##### Discrete Variable

- FullBath: Number of full bathrooms above ground

- HalfBath: Number of half bathrooms above ground

- BedroomAbvGr: Number of bedrooms above ground

- KitchensAbvGr: Number of kitchens above ground

- FirePlaces: Number of fireplaces

- YrBuilt: Original construction date

- YrSold: Year house was sold


```{r}
# load data
library(tidyverse)
dat <- read_csv("data/train.csv")
#Find numeric variables
numeric_var <- names(dat)[which(sapply(dat, is.numeric))]
#Find categorical variables
categorical_var <- names(dat)[-which(sapply(dat, is.numeric))]
#Numeric variables that we want to work with
var <- numeric_var[-c(1:3,5:6,8:12,14:16,18:19,24, 26:27,29:33,35:36)]

#drop all other variables from out dataset that we don't need.
#data with numerical variables of our choice
numeric_dat<-dat[var]
#data with categorical variables of our choice
cat_var<-categorical_var[c(9,29,35,38)]
categorical_dat<-dat[cat_var]
#final data with the 17 variables variables of our choice
final_dat <- cbind(numeric_dat,categorical_dat)
#In the dataset from Kaggle, PavedDrive is a categorical varible with three possible class: Y (if paved), P (if partially paved) and N(if dirt/gravel). We recode the variable so that PavedDrive is a binary variable that takes Y(Yes) if it is paved and N (No) otherwise.
final_dat$PavedDrive <- ifelse((final_dat$PavedDrive=="Y"), "Y", "N")
```

## EDA


#### Summary Statistics of Numeric Variables
```{r echo = F}
#Summary table for numeric variables
numeric_dat %>%
  summarise_all(list(mean = mean, std = sd), na.rm = TRUE) %>%
  pivot_longer(cols = everything(), 
               names_to = c('col', '.value'), 
               names_sep = '_')
```

#### Closer Look at Sale Price

```{r}
#The following code chunk takes a deeper look at the SalePrice feature:

#Histogram of house prices
final_dat  %>%
  ggplot(., aes(x = SalePrice)) +
  geom_histogram()

#Average house price over time
final_dat  %>%
  group_by(YrSold) %>%
  summarise(meanPrice = mean(SalePrice)) %>%
  ggplot(., aes(x = YrSold, y = meanPrice)) +
  geom_point() + geom_line()
```

Judging from the histogram, the distribution of SalePrice in our data set is slightly right skewed. This suggests that there are some outlier properties with significantly higher sale prices.

From the second plot,we can observe a  huge drop in average sale price of houses in our data. This coincides with the 2008 housing market crash, which suggests that there are some of the macroeconomic factors at play, such as the general state of the housing market at a national level. We could investigate the effect of 2008 housing market crash on housing sale prices by treating 'YrSold' as a categorical variable; however, for this project we choose not to do it and instead treat 'YrSold'as a numerical variable. This decision was based on our preference to work mostly with numeric variables, since statistical learning methods such as lasso regression and KNN classification, doesn't handle categorical variables well and the interpretation might get tricky. Having said that, the role of macroeconomic factors in determining sale price is worth addressing in our project limitations.


#### Correlation between some of the numeric and discrete data fields (most importantly SalePrice):

```{r}

correlation <- final_dat  %>%
  select(SalePrice, PoolArea, LotArea, TotalBsmtSF, GarageArea, GrLivArea, FullBath, HalfBath, BedroomAbvGr, KitchenAbvGr, Fireplaces,YrSold, YearBuilt)

corr_mtx <- cor(correlation, use = "pairwise.complete.obs")

ggcorrplot(corr_mtx, # input the correlation matrix
           hc.order = TRUE, 
           type = "upper", 
           lab = TRUE,
           method = "circle", 
           colors = c("tomato2", "white", "springgreen3"))
```
As seen from above, housing prices seem to be the most correlated (positively) with above ground living area (in sq. ft), total basement area(in sq. ft) and garage area (in sq. ft). The price is also moderately correlated with number of full bathrooms and year built.


#### Number of houses in different Neighborhoods:

```{r}
final_dat  %>%
  count(Neighborhood) %>%
  ggplot(aes(x = n, y = reorder(Neighborhood, n))) +
    geom_bar(stat = "identity")+
  labs(title = "Number of houses in each Neighborhood")
```
As seen from the plot, most houses in our dataset are located in NAmes, followed by CollegeCr and Old Town. Less than 50 houses are located in Mitchel, NoRidge, timber, IDOTRR,ClearCr,SWISU,StoneBr,MeadowV, Blmngtn, BrDale,Veenker, NPkVill and Blueste. This is something we will take into consideration when designing our regression and classification model.



## Methodology


### Ridge Regression







## Results

###Linear Regression
```{r}
#drop neighborhoods that had less than 50 houses
dataf<-final_dat[final_dat$Neighborhood != "Mitchel" & final_dat$Neighborhood != "NoRidge"
           & final_dat$Neighborhood != "Timber" & final_dat$Neighborhood != "IDOTRR"
           & final_dat$Neighborhood != "ClearCr" & final_dat$Neighborhood != "SWISU"
          & final_dat$Neighborhood != "StoneBr" & final_dat$Neighborhood != "MeadowV"
          & final_dat$Neighborhood != "Blmngtn" & final_dat$Neighborhood != "BrDale"
           & final_dat$Neighborhood != "Veenker" & final_dat$Neighborhood != "NPkVill"
          & final_dat$Neighborhood != "Blueste" , ] 

# create dummy variables for all categorical variables
dataf <- dummy_cols(dataf,remove_first_dummy = TRUE)
#drop the columns that stores categorical variable since thee same information 
# is stored in the newly created dummy variables
cat_name<-names(dataf)[-which(sapply(dataf, is.numeric))]
dataf <-dataf[,!(names(dataf) %in% cat_name)]
# for numeric variaables, the dataset has NA for 0. so replace NA with 0s.
dataf[is.na(dataf)] = 0
```

```{r}
set.seed(1)
n<-nrow(dataf)
# create vector of indices for each set
train_ids <- sample(1:n, .80*n)
test_ids <- (1:n)[-train_ids]
# create test and train set based on the indicators
train_set<-dataf[train_ids,]
test_set<-dataf[test_ids,]
model<-lm(SalePrice ~ ., train_set)
preds <- predict(model, test_set)
sqrt(mean((test_set$SalePrice-preds)^2))
coef(model)
```
Based on the output of the regression model, the 5 variables that seems to affect the sale price of the houses are 
KitchenAbvGr, Fireplace, Neighborhood_NridgHt (a dummy that indicates whether house is in 'NridgHt' neighborhood),
Neighborhood_Crawfor (a dummy that indicates whether house is in 'Crawfor' neighborhood and Neighborhood_Edwards (a dummy that indicates whether house is in 'Edwards' neighborhood.

**Interpretation of 5 coefficients highest in magnitude**

- Coefficient on KitchenAbvGr: Controlling for the  variables,including, but not limited to, lot area, garage finish type, central air conditionaing and neighborhood, an increase in number of kitchen by 1 decreases the saleprice by $33924.08 .

-Coefficient on Fireplace: Controlling for the  variables,including, but not limited to, lot area, garage finish type, central air conditionaing and neighborhood, a unit increase in number of fireplaces increases the saleprice by $10022.39.

-Coefficient on Neighborhood_NridgH: Controlling for the  variables,including, but not limited to, lot area, garage finish type, central and air conditioning, a house in NridgH neighborhood is pricier by $76357.29 compared to a house in BrkSide Neighborhood.

-Coefficient on Neighborhood_Crawfor: Controlling for the  variables,including, but not limited to, lot area, garage finish type, central and air conditioning, a house in Crawfor neighborhood is pricier by $27039.75 compared to a house in BrkSide Neighborhood.

- Coefficient on Neighborhood_Edwards: Controlling for the  variables,including, but not limited to, lot area, garage finish type, central and air conditioning, a house in Edwards neighborhood is cheaper by $16307.46 compared to a house in BrkSide Neighborhood.


The RMSE error is 28837.11. This implies that the prediction of houses in the test set is,on average, off by $28837.11. This error seems small given that the the sale peice of the houses are in millions of dollar.

### Ridge Regression

```{r}
library(glmnet)
x <- model.matrix(SalePrice ~ ., dataf)[,-1]
y<-dataf$SalePrice
n <- nrow(dataf)
```

```{r}
set.seed(10)
y_test<-y[test_ids]
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 0, nfolds = 10)
best_lam <- cv_out$lambda.min
#find RMSE
grid <- 10^seq(5,-2,length = 100)
ridge_mod <- glmnet(x[train_ids,], y[train_ids], alpha = 0, lambda = grid)
ridge_preds <- predict(ridge_mod, s= best_lam, newx = x[test_ids,])
sqrt(mean((ridge_preds - y_test)^2))
ridge_final <- glmnet(x, y, alpha = 0, lambda = best_lam)
round(coef(ridge_final), 3)
```
Based on the output of the regression model, the 5 variables that seems to affect the sale price of the houses are 
KitchenAbvGr, Neighborhood_Somerst(a dummy that indicates whether house is in 'Somerst' neighborhood)  , Neighborhood_NridgHt (a dummy that indicates whether house is in 'NridgHt' neighborhood),Neighborhood_Crawfor (a dummy that indicates whether house is in 'Crawfor' neighborhood and Neighborhood_Edwards (a dummy that indicates whether house is in 'Edwards' neighborhood.

**Interpretation of 5 coefficients highest in magnitude**

- Coefficient on KitchenAbvGr: Controlling for the  variables,including, but not limited to, lot area, garage finish type, central air conditionaing and neighborhood, an increase in number of kitchen by 1 decreases the saleprice by $23577.447 .

-Coefficient on Neighborhood_NridgH: Controlling for the  variables,including, but not limited to, lot area, garage finish type, central and air conditionaing, a house in NridgH neighborhood is pricier by $58767.440 compared to a house in BrkSide Neighborhood.

-Coefficient on Neighborhood_Crawfor: Controlling for the  variables,including, but not limited to, lot area, garage finish type, central and air conditioning, a house in Crawfor neighborhood is pricier by $ 22619.590 compared to a house in BrkSide Neighborhood.

- Coefficient on Neighborhood_Edwards: Controlling for the  variables,including, but not limited to, lot area, garage finish type, central and air conditioning, a house in Edwards neighborhood is cheaper by $16307.46 compared to a house in BrkSide Neighborhood.

 - Coefficient on Neighborhood_Somerst: Controlling for the  variables,including, but not limited to, lot area, garage finish type, central and air conditioning, a house in Somerst neighborhood is cheaper by $15562.545 compared to a house in BrkSide Neighborhood.


The RMSE error is 29680.6. This implies that the prediction of houses in the test set is,on average, off by $29680.6. This error seems small given that the the sale peice of the houses are in millions of dollar.



### Lasso Regression

```{r}

cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 1, nfolds = 10)
best_lam <- cv_out$lambda.min
grid <- 10^seq(5,-2,length = 100)

#coefficients
lasso_mod <- glmnet(x[train_ids,], y[train_ids],alpha = 1, lambda = grid)
lasso_coef <- predict(lasso_mod, type = "coefficients", s = best_lam)#[1:18,]
lasso_coef
#RMSE
sqrt(mean((predict(lasso_mod, s= best_lam, newx = x[test_ids,]) - y_test)^2))
```
Based on the output, the lasso model sclaed the coefficients on LotArea,HalfBath, BedRoomAbvGr, Neighborhood_Gilbert, Neighborhood_NWAmes, GarageFinish_NA and PavedDrive_Y to 0. The interpretation of the coefficients of Neighborhood_Gilbert, Neighborhood_NWAmes, GarageFinish_NA scaled to 0 is a bit trickier  since neighborhood and GarageFinish were categorical variables with multiple classes but only coefficients of certain classes were scaled to 0. It is hard to tell whether this implies neighborhood and garage finish are not important covariates or only certain classes of these categorical variables are important when predicting for saleprice.

Similar to Ridge Regressionl, the 5 variables that seems to affect the sale price of the houses the most in the Lasso model are KitchenAbvGr, Neighborhood_Somerst(a dummy that indicates whether house is in 'Somerst' neighborhood)  , Neighborhood_NridgHt (a dummy that indicates whether house is in 'NridgHt' neighborhood),Neighborhood_Crawfor (a dummy that indicates whether house is in 'Crawfor' neighborhood and Neighborhood_Edwards (a dummy that indicates whether house is in 'Edwards' neighborhood. The interpretation is similar to that of ridge regression.

The RMSE error is 28752.86. This implies that the prediction of houses in the test set is,on average, off by $29680.6. This error seems small given that the the sale price of the houses are in millions of dollar.

#KNN
```{r}
library(class)
classification_dat<-final_dat[final_dat$Neighborhood != "Mitchel" & final_dat$Neighborhood != "NoRidge"
           & final_dat$Neighborhood != "Timber" & final_dat$Neighborhood != "IDOTRR"
           & final_dat$Neighborhood != "ClearCr" & final_dat$Neighborhood != "SWISU"
          & final_dat$Neighborhood != "StoneBr" & final_dat$Neighborhood != "MeadowV"
          & final_dat$Neighborhood != "Blmngtn" & final_dat$Neighborhood != "BrDale"
           & final_dat$Neighborhood != "Veenker" & final_dat$Neighborhood != "NPkVill"
          & final_dat$Neighborhood != "Blueste" , ] 
classification_dat<-final_dat%>%
  mutate(CentralAir=ifelse(final_dat$CentralAir=="Y", 1, 0),
         PavedDrive=ifelse(final_dat$PavedDrive=="Y", 1, 0))%>%
  select(-GarageFinish)

classification_dat_scale<- classification_dat %>%
  mutate_if(is.numeric, scale)
```

```{r}
train_set<-classification_dat_scale[train_ids,]
test_set<-classification_dat_scale[test_ids,]

knn_preds <- knn(train = train_set%>%dplyr::select(-Neighborhood),,
                 test = test_set%>% dplyr::select(-Neighborhood),,
                 cl =train_set$Neighborhood, 
                 k = 10)
#table(pred = knn_preds, truth = test_set$Neighborhood)
mean(test_set$Neighborhood != knn_preds)
```


#Naive Bayes
```{r}
library(e1071)
#final_dat$GarageFinish[is.na(final_dat$GarageFinish)] <- "NoGrg"
#train_set<-final_dat[train_ids,]
#test_set<-final_dat[test_ids,]
# Fitting Naive Bayes Model
set.seed(12)  # Setting Seed

classifier_cl <- naiveBayes(Neighborhood ~ .,train_set)
#classifier_cl
 
# Predicting on test data'
y_pred <- predict(classifier_cl, newdata = test_set)
 
# Confusion Matrix
cm <- table(test_set$Neighborhood, y_pred)
 
# Model Evaluation
mean(test_set$Neighborhood != y_pred)
```




###Tree
```{r}
#final_dat$GarageFinish[final_dat$GarageFinish == 'NA'] <- 'NoGrg'
#final_dat$GarageFinish[is.na(final_dat$GarageFinish)] <- "NoGrg"
#final_dat$CentralAir <- ifelse(final_dat$CentralAir=="Y", 1, 0)
#final_dat$PavedDrive <- ifelse(final_dat$PavedDrive=="Y", 1, 0)
```


```{r}
library(tree)
classification_dat_scale <- classification_dat_scale %>%
 mutate_if(is.character, factor)
train_set<-classification_dat_scale[train_ids,]
test_set<-classification_dat_scale[test_ids,]
#final_dat$GarageFinish<-as.factor(final_dat$GarageFinish)
#final_dat$CentralAir<-as.factor(final_dat$CentralAir)
#final_dat$PavedDrive<-as.factor(final_dat$PavedDrive)
#final_dat$Neighborhood<-as.factor(final_dat$Neighborhood)
#final_dat$YearBuilt<-as.factor(final_dat$YearBuilt)

#train_set<-final_dat[train_ids,]
#test_set<-final_dat[test_ids,]
tree_house<-tree(Neighborhood ~ .,train_set)
summary(tree_house)
plot(tree_house)
text(tree_house,pretty=0,cex=.5)
#tree_preds <- predict(tree_house, test_set, type = "class")
#mean(test_set$Neighborhood != tree_preds)

```
```{r}
dat %>%
 group_by(Neighborhood) %>%
summarise(count = n())%>%
  arrange(count)
```

### XgBoost

```{r}
library(xgboost)
```

```{r}
# Convert the Species factor to an integer class starting at 0
# This is picky, but it's a requirement for XGBoost
neighborhood <- final_dat$Neighborhood
label <- as.integer(final_dat$Neighborhood)-1
final_dat$Neighborhood <- NULL
xgboost_dat<-final_dat%>%
  mutate(CentralAir=ifelse(final_dat$CentralAir=="Y", 1, 0),
         PavedDrive=ifelse(final_dat$PavedDrive=="Y", 1, 0))
#final_dat$CentralAir <- ifelse(final_dat$CentralAir=="Y", 1, 0)
#final_dat$PavedDrive <- ifelse(final_dat$PavedDrive=="Y", 1, 0)
xgboost_dat <-xgboost_dat %>%
  select(-GarageFinish)
train_set = as.matrix(xgboost_dat [train_ids,])
train_label = label[train_ids]
test_set = as.matrix(xgboost_dat [-train_ids,])
test_label = label[-train_ids]
```

```{r}
xgb_train = xgb.DMatrix(data=train_set,label=train_label)
xgb_test = xgb.DMatrix(data=test_set,label=test_label)
```

```{r}
# Define the parameters for multinomial classification
num_class = length(levels(neighborhood))
params = list(
  booster="gbtree",
  eta=0.001,
  max_depth=5,
  gamma=3,
  subsample=0.75,
  colsample_bytree=1,
  objective="multi:softprob",
  eval_metric="mlogloss",
  num_class=num_class
)
```


```{r}
xgb.fit=xgb.train(
  params=params,
  data=xgb_train,
  nrounds=1000,
  early_stopping_rounds=10,
  watchlist=list(val1=xgb_train,val2=xgb_test),
  verbose=0
)
```


```{r}
xgb.pred = predict(xgb.fit,test_set,reshape=T)
xgb.pred = as.data.frame(xgb.pred)
colnames(xgb.pred) = levels(neighborhood)

# Use the predicted label with the highest probability
xgb.pred$prediction = apply(xgb.pred,1,function(x) colnames(xgb.pred)[which.max(x)])
xgb.pred$label = levels(neighborhood)[test_label+1]

# Calculate the final accuracy
result = sum(xgb.pred$prediction==xgb.pred$label)/nrow(xgb.pred)
```




## Discussion