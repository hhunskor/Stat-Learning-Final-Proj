---
title: "Math 218: Final Report"
subtitle: "Project Haydoja"
author: "Payoja and Hayden"
output: pdf_document
date: "Dec 6"
editor_options: 
chunk_output_type: inline
---

```{r Load Packages}
library('fastDummies')
library(tidyverse)
library(dplyr)
library(tidyr)
library(ggcorrplot)
```

## Introduction

There is a plethora of statistical tools available to understand any given set of data. Data scientists and statisticians often use either supervised or unsupervised statistical learning methods to model their data to both better understand trends and extrapolate predictions. For the our project, however, we decided to exclusively focus on supervised learning, mainly for two reasons. First, supervised learning methods are always guided by a response variable which ensures our research objective is clearly defined. Second, supervised learning models have relevant performance metrics (i.e. RMSE for regression problems or misclass. rate for classification problems).

For this project, we decided to implement a variety of supervised learning techniques using the Ames Housing dataset, which is a publicly available dataset that includes extensive information on the sale of residential properties in Ames, Iowa from 2006 to 2010. It contains 1460 observations with 79 variables on various house specifications. The variables are a mix of continuous, categorical and discrete types.

The data is available on [Kaggle](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data).

We will investigate both regression and classification problems, with the primary goal of better understanding which statistical learning method perform best on different features from the Ames housing dataset. Below are our driving research questions:

1. Which statistical learning method most accurately predicts housing prices using other available housing features?

2. Which statistical learning method most accurately classifies houses by neighborhood based on the other available housing features?

Although our primary goal is to identify which statistical learning method performs best, we will also spend some time exploring and understanding which predictors affect our response variables and how.

### Data Description

```{r Load Data}
original_dat <- read_csv("data/train.csv")
```

Rather than use all 79 available variables, we decided to only include features we thought were particularly relevant to our two driving research questions. This dataset also includes a lot of categorical variables, so we decided to only include the ones that can be easily translated to usable dummy/binary variables for the sake of simplicity.

Below is a description of the features we focused on for this project:

##### Categorical Variables:

- Neighborhood: Physical locations within Ames city limits
- CentralAir: Central air conditioning (Yes/No)
- PavedDrive: Paved driveway (Yes/No)
- GarageFinish: Interior finish of the garage
- BldgType: Type of dwelling
- HouseStyle: Style of dwelling
- BsmtFinType1: Rating of basement finished area

##### Continuous Variables:

- SalePrice: Price of house 
- PoolArea: Pool area in square feet
- LotArea: Lot size in square feet
- TotalBsmtSF: Total square feet of basement area
- GarageArea: Size of garage in square feet
- GrLivArea: Above ground living area square feet

##### Discrete Variable

- FullBath: Number of full bathrooms above ground
- HalfBath: Number of half bathrooms above ground
- BedroomAbvGr: Number of bedrooms above ground
- KitchenAbvGr: Number of kitchens above ground
- Fireplaces: Number of fireplaces
- OverallQual: Rates the overall material and finish of the house
- OverallCond: Rates the overall condition of the house
- YearBuilt: Original construction date
- YearRemodAdd: Year house was remodeled
- YrSold: Year house was sold

One caveat to this trimmed feature list is that there is a feature 'SaleCondition' that defines the sale of a house as either "normal" or one of several types of abnormal sales. We only want to look at the normal sales, so we've selected only those observations with SaleCondition = "normal":

```{r Filtering Abnormal Sales}
original_dat <- filter(original_dat, SaleCondition == "Normal")
```

The code chunk below further trims our original data to include just the features we are interested in:

```{r Trim Features}
dat <- original_dat %>%
  select(Neighborhood, CentralAir, PavedDrive, GarageFinish, BldgType, HouseStyle, BsmtFinType1,
         SalePrice, PoolArea, LotArea, TotalBsmtSF, GarageArea, GrLivArea, FullBath, HalfBath,
         BedroomAbvGr, KitchenAbvGr, Fireplaces, OverallQual, OverallCond, YearBuilt, YearRemodAdd, YrSold, PavedDrive)
```

## EDA

#### Investigating SalePrice Variable

The first exploratory question we wanted to investigate is if there is, in fact, a correlation between how big a house is and how much it costs:

```{r LotArea vs. SalePrice}
ggplot(dat, aes(x = LotArea, y = SalePrice)) +
  geom_point()
```

This plot suggests that, although there may be a slight positive correlation between the LotArea and SalePrice, there are likely other variables that contribute to how expensive a house is.

Let's now look a bit more at the SalePrice variable and its distribution:

```{r Sale Price EDA}
#Histogram of house prices
dat %>%
  ggplot(., aes(x = SalePrice)) +
  geom_histogram()

#Average house price over time
dat %>%
  group_by(YrSold) %>%
  summarise(meanPrice = mean(SalePrice)) %>%
  ggplot(., aes(x = YrSold, y = meanPrice)) +
  geom_point() + geom_line()
```

Judging from the histogram, the distribution of SalePrice in our data set is slightly right skewed. This suggests that there are some outlier properties with significantly higher sale prices. To account for this, we will add a logarithmic transformation to the SalePrice variable (see data cleaning section).

In the second plot, we can observe a huge drop in average sale price of houses in our data coinciding with the 2008 housing market crash. To try to account for this, we decided to replace YrSold with a binary variable 'Sold_08' that will be Y if a house was sold in 2008, and N otherwise:

```{r Accounting for Housing Crash}
dat$Sold_08 <- as.factor(ifelse(dat$YrSold == '2008', 'Y', 'N'))
dat <- select(dat, -YrSold)
```

#### Investigating Neighborhood Variable

We also wanted to look at the distribution of houses across neighborhoods, as well as how expensive the homes are in each of the neighborhoods:

```{r Distribution of Houses By Neighborhood}
dat %>%
  count(Neighborhood) %>%
  ggplot(aes(x = n, y = reorder(Neighborhood, n))) +
  geom_bar(stat = "identity") +
  labs(title = "Number of Houses in Each Neighborhood",
       x = "Number of Houses",
       y = "Neighborhood")
```

This plot reveals that there is a pretty big gradient of neighborhood sizes. To simplify our models and their interpretations, we want to reduce the number of neighborhood categories by grouping some of the neighborhoods into an 'Other' category.

To do so, we first isolated the neighborhoods with fewer than 50 homes:

```{r}
other_neighborhoods <- dat %>%
  group_by(Neighborhood) %>%
  filter(n() < 50) %>%
  distinct(Neighborhood)
```

We then looked at how these 16 neighborhoods vary in terms of average price and average house age:

```{r}
dat %>%
  filter(dat$Neighborhood %in% other_neighborhoods$Neighborhood) %>%
  group_by(Neighborhood) %>%
  summarise(avg_built_date = mean(YearBuilt),
            mean = mean(SalePrice)) %>%
  arrange(-avg_built_date) %>%
  ggplot(aes(x = avg_built_date, y = mean)) +
  geom_text(aes(label = Neighborhood))
```

Right off the bat, we decided to merge NoRidge (Northridge) and NridgHt (Northridge Heights) because they both are comprised primarily of new, expensive homes. Also, a quick Google search revealed that the two neighborhoods border each other geographically. We defined this combined neighborhood as 'GrNoRidge' or Greater Northridge:

```{r}
dat$Neighborhood <- ifelse(dat$Neighborhood %in% c('NoRidge', 'NridgHt'), 'GrNoRidge', dat$Neighborhood)
```

Other than that, there weren't really any other neighborhood merges that made sense geographically. And if we were to merge all the neighborhoods with < 50 houses, we would get an 'Other' neighborhood category that accounts for the second highest number of houses in the dataset. This would likely skew how our model predicts neighborhood classification.

So, we decided look at how much data we would be losing if we just dropped the neighborhoods with < 50 houses or < 30 houses:

```{r}
nrow(filter(dat, dat$Neighborhood %in% other_neighborhoods$Neighborhood))

other_neighborhoods <- dat %>%
  group_by(Neighborhood) %>%
  filter(n() < 30) %>%
  distinct(Neighborhood)

nrow(filter(dat, dat$Neighborhood %in% other_neighborhoods$Neighborhood))
```

We decided to go ahead and drop the 180 houses in neighborhoods with < 30 houses:

```{r}
dat <- dat %>%
  filter(! Neighborhood %in% other_neighborhoods$Neighborhood)
```

#### Correlation between some of the numeric and discrete data fields (most importantly SalePrice):

```{r Numeric Correlation}
correlation <- dat %>%
  select(SalePrice, PoolArea, LotArea, TotalBsmtSF, GarageArea, GrLivArea, FullBath, HalfBath,
         BedroomAbvGr, KitchenAbvGr, Fireplaces, YearBuilt, OverallQual, OverallCond)

corr_mtx <- cor(correlation, use = "pairwise.complete.obs")

ggcorrplot(corr_mtx, # input the correlation matrix
           hc.order = TRUE, 
           type = "upper", 
           lab = TRUE,
           method = "circle", 
           colors = c("tomato2", "white", "springgreen3"))
```

As seen from above, housing prices seem to be the most correlated (positively) with overal quality of the house, above ground living area (in sq. ft), total basement area (in sq. ft) and garage area (in sq. ft). The price is also moderately correlated with number of full bathrooms.

## Data Cleaning and Scaling

#### Feature Engineering

There are a couple problems with the way our variables are formatted. First, if a house has not been remodeled, the YearRemodAdd will be the same as YearBuilt. Let's look at how many houses this case applies to:

```{r}
nrow(filter(dat, YearRemodAdd == YearBuilt))
```

Since over half the houses have never been remodeled, we just decided to convert YearRemodAdd to a binary variable:

```{r Remodel Variable}
dat$Remod <- as.factor(ifelse(dat$YearBuilt == dat$YearRemodAdd, 'Y', 'N'))
dat <- select(dat, -YearRemodAdd)
```

Additionally, PavedDrive is a categorical variable with three possible class: Y (if paved), P (if partially paved) and N (if dirt/gravel). For simplicity, we decided to recode the variable so that PavedDrive is a binary variable that takes Y(Yes) if it is paved and N (No) otherwise.

```{r PavedDrive Variable}
dat$PavedDrive <- ifelse((dat$PavedDrive == "Y"), "Y", "N")
```

#### Scaling/Transformations

Here is where we want to apply the log transformation to our SalePrice variable:

```{r SalePrice Log Transform}
dat$SalePrice <- log(dat$SalePrice)
```

Now, we want to scale all of our numeric variables:

```{r Scaling}

#First save mean and sd of SalePrice for un-scaling
#SP_mean <- mean(dat$SalePrice)
#SP_std <- sd(dat$SalePrice)

num_vars <- names(dat %>% select_if(is.numeric))
num_vars <- num_vars[! num_vars == "SalePrice"]
dat[, num_vars] <- scale(dat[, num_vars], center=TRUE, scale=TRUE)
```


#### Handling NAs

```{r}
sapply(dat, function(x) sum(is.na(x)))

dat$GarageFinish <- ifelse(is.na(dat$GarageFinish), "NoGar", dat$GarageFinish)
dat$BsmtFinType1 <- ifelse(is.na(dat$BsmtFinType1), "NoBsmt", dat$BsmtFinType1)

```


## Methodology

To answer our first research question relating to the regression problem, "Which statistical learning method most accurately predicts housing prices using other available housing features?", we decided to use three different regression models: Linear Regression, Ridge Regression and Lasso Regression. For each of the these three methods we divide our data into two sets 'train' and 'test'. The 'train' set contains 80% of our entire data where as the 'test' set contains the remaining 20%. The train and the test set are the same for all three regressions. The response variable is SalePrice and the predictors are all the other 22 variables listed in the data description section.A brief description on each of these methods is below.

**1. Linear Regression**
Our linear model will include both numeric and categorical variable. For categorical variables, we create dummy variables using the 'fastDummies' package. To validate the performance of our model, we  predict the sale price of houses in the test set using the lasso model fit on the training set. Then, we will examine the test error rate by measuring the root mean squares error. 

**2. Ridge Regression**
We implement the ridge regression to examine whether shrinking features that are not as important in determining sales price improves the performance of the model. For categorical variables, we create dummy variables. Using the 22 predictors,we fit the ridge model on the training set,with  with $\lambda$ chosen by cross-validation with k-fold equals 10. To validate the performance of our model, we  predict the sale price of houses in the test set using the lasso model fit on the training set. Then, we will examine the test error rate by measuring the root mean squares error. The R package required to implement this method is 'glmnet'.

**3. Lasso Regresison**
22 predictors, although better than having too few predictors, may be too many to use for this research question. Thus, we want to implement the lasso method so that coefficients of features that are not as important shrinks to 0. For categorical variables, we create dummy variables. We realize that interpreting this might be tricky as only coefficient on some of the classes might be shrunk to 0, but we believe features such as neighborhood are important in determining the sale price. Thus, we chose to create dummies instead of getting rid of them.Using the 22 predictors,we fit the lasso model on the training set,with  with $\lambda$ chosen by cross-validation with k-fold equals 10. To validate the performance of our model, we predict the sale price of houses in the test set using the lasso model fit on the training set. Then, we will examine the test error rate by measuring the root mean squares error. The R package required to implement this method is 'glmnet'.

**4. XgBoost**
-----

To answer our second research question relating to the classification problem, "Which  statistical learning method most accurately classifies houses by neighborhood based on the other available housing features?", we decided to use three different classification models: Naive Beyes, Tree-Based Classification and XgBoost. For each of the these three methods we divide our data into two sets 'train' and 'test'. The 'train' set contains 80% of our entire data where as the 'test' set contains the remaining 20%. The train and the test set are the same for all three Classification methods The response variable is Neighborhood and the predictors are YrBuilt, YearRemodAdd, SalePrice,**Don't remember all the variable names that you mentioned**.A brief description on each of these methods is below.

**1. Naive Bayes**
To implement Naive Bayes, we use the naiveBayes() function, which is part of the e1071 library, on our test set. To evaluate the performance of our model, we will predict the response on the test data and produce a contingency table comparing the true test labels to the predictions. The R package required to implement this method is 'e1071'.

**2. Tree-Based Classification**
To implement this method, we will fit a decision classification tree to the training data, using 'neighborhood' as the response variable. For predictors, we will use all other variables. To evaluate the performance of our model, we will predict the response on the test data and produce a contingency table comparing the true test labels to the predictions. The R package required to implement this method is 'tree'. 

**3. XgBoost**



## Results

### Which statistical learning method most accurately predicts housing prices using other available housing features?

####Linear Regression

```{r Basic Linear Regression}
set.seed(1)
n <- nrow(dat)
train_ids <- sample(1:n, .80*n)
test_ids <- (1:n)[-train_ids]

mod1 <- lm(SalePrice ~ ., dat[train_ids,])

#Make predictions
mod1_preds <- predict(mod1, dat[test_ids,])

#RMSE
sqrt(mean((exp(dat$SalePrice[test_ids]) - exp(mod1_preds)) ^ 2))
```


```{r CV Linear Regression}
set.seed(1)
K <- 10

# randomly split the indices/observations into K groups of roughly equal size
rand <- sample(1:nrow(dat))
group_ids <- split(rand, cut(seq_along(rand), K, labels = FALSE))

rmse_vec <- rep(NA,K)
n <- nrow(dat)

for(i in 1:K){
  test_ids <- group_ids[[i]]
  train_ids <- (1:n)[-test_ids]
  
  train_mod <- lm(SalePrice ~ ., data = dat[train_ids, ])
  pred <- predict(train_mod, newdata = dat[test_ids,])
  mse_i <- (exp(dat$SalePrice[test_ids]) - exp(pred))^2
  rmse_vec[i] <- sqrt(mean(mse_i))
}

mean(rmse_vec)
```



```{r}
#drop neighborhoods that had less than 50 houses
dataf<-final_dat[final_dat$Neighborhood != "Mitchel" & final_dat$Neighborhood != "NoRidge"
           & final_dat$Neighborhood != "Timber" & final_dat$Neighborhood != "IDOTRR"
           & final_dat$Neighborhood != "ClearCr" & final_dat$Neighborhood != "SWISU"
          & final_dat$Neighborhood != "StoneBr" & final_dat$Neighborhood != "MeadowV"
          & final_dat$Neighborhood != "Blmngtn" & final_dat$Neighborhood != "BrDale"
           & final_dat$Neighborhood != "Veenker" & final_dat$Neighborhood != "NPkVill"
          & final_dat$Neighborhood != "Blueste" ] 

# create dummy variables for all categorical variables
dataf <- dummy_cols(dat,remove_first_dummy = TRUE)
#drop the columns that stores categorical variable since thee same information 
# is stored in the newly created dummy variables
cat_name<-names(dataf)[-which(sapply(dataf, is.numeric))]
dataf <-dataf[,!(names(dataf) %in% cat_name)]
# for numeric variaables, the dataset has NA for 0. so replace NA with 0s.
dataf[is.na(dataf)] = 0
```

```{r}
set.seed(3)
n<-nrow(dataf)
# create vector of indices for each set
train_ids <- sample(1:n, .80*n)
test_ids <- (1:n)[-train_ids]
# create test and train set based on the indicators
train_set<-dataf[train_ids,]
test_set<-dataf[test_ids,]
model<-lm(SalePrice ~ ., train_set)
preds <- predict(model, test_set)
sqrt(mean((exp(test_set$SalePrice)-exp(preds))^2))
coef(model)
```
Based on the output of the regression model, the 5 variables that seems to affect the sale price of the houses are 
 Neighborhood_NridgHt (a dummy that indicates whether house is in 'NridgHt' neighborhood),
Neighborhood_Crawfor (a dummy that indicates whether house is in 'Crawfor' neighborhood and Neighborhood_Somerst (a dummy that indicates whether house is in 'Somerst' neighborhood,BldgType_Twnhs (a dummy that indicats whether the building type is a townhouse end unit or not) and BldgType_TwnhsE (a dummy that indicats whether the building type is a townhouse inside unit or not).

**Interpretation of 5 coefficients highest in magnitude**

-Coefficient on Neighborhood_NridgH: Controlling for the  variables,including, but not limited to, lot area, garage finish type, central  air conditioning, a house in NridgH neighborhood is pricier by $69648.21 compared to a house in BrkSide Neighborhood.

-Coefficient on Neighborhood_Crawfor: Controlling for the  variables,including, but not limited to, lot area, garage finish type, central air conditioning, a house in Crawfor neighborhood is pricier by $19922.71 compared to a house in BrkSide Neighborhood.

- Coefficient on Neighborhood_Somerst: Controlling for the  variables,including, but not limited to, lot area, garage finish type, central air conditioning, a house in Somerst neighborhood is pricier by $119817.21 compared to a house in BrkSide Neighborhood.

- Coefficient on BldgType_Twnhs: Controlling for the  variables,including, but not limited to, lot area, garage finish type, central air conditioning, a townhouse end unit is cheaper by $62914.39 compared to  a single-family detached.	

- Coefficient on BldgType_TwnhsE: Controlling for the  variables,including, but not limited to, lot area, garage finish type, central air conditioning, a townhouse end unit is cheaper by $33720.04 compared to  a single-family detached.


The RMSE error is 24588.56. This implies that the prediction of houses in the test set is,on average, off by $24588.56. This error seems small given that the the sale price of the houses are much higer.

#### Ridge Regression

```{r}
library(glmnet)
x <- model.matrix(SalePrice ~ ., dataf)[,-1]
y<-dataf$SalePrice
n <- nrow(dataf)
```

```{r}
#set.seed(3)
y_test<-y[test_ids]
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 0, nfolds = 10)
best_lam <- cv_out$lambda.min
#find RMSE
grid <- 10^seq(5,-2,length = 100)
ridge_mod <- glmnet(x[train_ids,], y[train_ids], alpha = 0, lambda = grid)
ridge_preds <- predict(ridge_mod, s= best_lam, newx = x[test_ids,])
sqrt(mean((exp(ridge_preds) - exp(y_test))^2))
ridge_final <- glmnet(x, y, alpha = 0, lambda = best_lam)
round(coef(ridge_final), 3)
```
Based on the output of the regression model, the 5 variables that seems to affect the sale price of the houses are 
 Neighborhood_NridgHt (a dummy that indicates whether house is in 'NridgHt' neighborhood),
Neighborhood_Crawfor (a dummy that indicates whether house is in 'Crawfor' neighborhood and Neighborhood_Somerst (a dummy that indicates whether house is in 'Somerst' neighborhood,BldgType_Twnhs (a dummy that indicats whether the building type is a townhouse end unit or not) and BldgType_TwnhsE (a dummy that indicats whether the building type is a townhouse inside unit or not). The interpretation is similar to that of linear regression regression.


The RMSE error is 25638.98. This implies that the prediction of houses in the test set is,on average, off by $25638.98. This error seems small given that the the sale peice of the houses are much higer.

#### Lasso Regression

```{r}

cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 1, nfolds = 10)
best_lam <- cv_out$lambda.min
grid <- 10^seq(5,-2,length = 100)

#coefficients
lasso_mod <- glmnet(x[train_ids,], y[train_ids],alpha = 1, lambda = grid)
lasso_coef <- predict(lasso_mod, type = "coefficients", s = best_lam)#[1:18,]
lasso_coef
#RMSE
sqrt(mean((exp(predict(lasso_mod, s= best_lam, newx = x[test_ids,])) - exp(y_test))^2))
```
Based on the output, the lasso model sclaed the coefficients on LotArea,HalfBath, BedRoomAbvGr, YrSold, Neighborhood_Gilbert,Neighborhood_NAmes, Neighborhood_NWAmes,Neighborhood_Sawyer,Neighborhood_SawyerW, BldgType_2fmCon, HouseStyle_2.5Unf,HouseStyle_2Story,HouseStyle_SFoyer,HouseStyle_SLvl, BsmtFinType1_BLQ, BsmtFinType1_LwQ, BsmtFinType1_Rec, BsmtFinType1_NA, CentralAir_Y, GarageFinish_NA and PavedDrive_Y to 0. The interpretation of the coefficients of some of the categories in Neighborhoods, Building Type, HouseStyle, Basement Finish Type and Garage finish getting scaled to 0 is a bit trickier  since these variables were categorical variables with multiple classes but only coefficients of certain classes were scaled to 0. It is hard to tell whether this implies these variables are not important covariates or only certain classes of these categorical variables are important when predicting for saleprice.

Similar to Linear ans Ridge Regression, the 5 variables that seems to affect the sale price of the houses the most in the Lasso model are Neighborhood_NridgHt (a dummy that indicates whether house is in 'NridgHt' neighborhood),
Neighborhood_Crawfor (a dummy that indicates whether house is in 'Crawfor' neighborhood and Neighborhood_Somerst (a dummy that indicates whether house is in 'Somerst' neighborhood,BldgType_Twnhs (a dummy that indicats whether the building type is a townhouse end unit or not) and BldgType_TwnhsE (a dummy that indicats whether the building type is a townhouse inside unit or not). The interpretation is similar to that of linear regression.

The RMSE error is 25077.4. This implies that the prediction of houses in the test set is,on average, off by $25077.4. This error seems small given that the the sale price of the houses are much higer.

#### XGBoost

```{r}
set.seed(3)
n <- nrow(dat)

#Create vector of indices for each set
train_ids <- sample(1:n, .80*n)
test_ids <- (1:n)[-train_ids]

#Create dataset that XGBoost can handle
xg_dat <- dummy_cols(dat, remove_first_dummy = TRUE)

#Drop old categorical columns
cat_name <- names(xg_dat)[-which(sapply(xg_dat, is.numeric))]
xg_dat <- xg_dat[,!(names(xg_dat) %in% cat_name)]

#Replace numeric NAs with 0s.
xg_dat[is.na(xg_dat)] = 0

#Create test and train sets
train_x = data.matrix(select(xg_dat[train_ids,], -SalePrice))
train_y = xg_dat[train_ids,]$SalePrice

test_x = data.matrix(select(xg_dat[test_ids,], -SalePrice))
test_y = xg_dat[test_ids,]$SalePrice

xgb_train = xgb.DMatrix(data = train_x, label = train_y)
xgb_test = xgb.DMatrix(data = test_x, label = test_y)

#Create model
xgbc = xgboost(data = xgb_train, max.depth = 2, nrounds = 100) #We arbitrarily chose to run this for 100 rounds
```

Note: it is hard to understand what exactly the train RMSE values here mean just because they are calculated based on the log-transformed SalePrice value. We can convert the test RMSE values back to USD values using the exp() function:

```{r}
pred_y = predict(xgbc, xgb_test)
sqrt(mean((exp(test_y) - exp(pred_y))^2))
```

### Which  statistical learning method most accurately classifies houses by neighborhood based on the other available housing features?

#### Naive Bayes
```{r}
library(e1071)

# Fitting Naive Bayes Model
set.seed(3)  # Setting Seed

classifier_cl <- naiveBayes(Neighborhood ~ ., dat[train_ids,])
#classifier_cl
 
# Predicting on test data'
y_pred <- predict(classifier_cl, newdata = dat[test_ids,])
 
# Confusion Matrix
cm <- table(dat[test_ids,]$Neighborhood, y_pred)
 
# Model Evaluation
mean(dat[test_ids,]$Neighborhood != y_pred)
```

####Tree
```{r}
#final_dat$GarageFinish[final_dat$GarageFinish == 'NA'] <- 'NoGrg'
#final_dat$GarageFinish[is.na(final_dat$GarageFinish)] <- "NoGrg"
#final_dat$CentralAir <- ifelse(final_dat$CentralAir=="Y", 1, 0)
#final_dat$PavedDrive <- ifelse(final_dat$PavedDrive=="Y", 1, 0)
```


```{r}
library(tree)
classification_dat_scale <- classification_dat_scale %>%
 mutate_if(is.character, factor)
train_set<-classification_dat_scale[train_ids,]
test_set<-classification_dat_scale[test_ids,]
#final_dat$GarageFinish<-as.factor(final_dat$GarageFinish)
#final_dat$CentralAir<-as.factor(final_dat$CentralAir)
#final_dat$PavedDrive<-as.factor(final_dat$PavedDrive)
#final_dat$Neighborhood<-as.factor(final_dat$Neighborhood)
#final_dat$YearBuilt<-as.factor(final_dat$YearBuilt)

#train_set<-final_dat[train_ids,]
#test_set<-final_dat[test_ids,]
tree_house<-tree(as.factor(Neighborhood) ~ .,dat[train_ids,])
summary(tree_house)
plot(tree_house)
text(tree_house,pretty=0,cex=.5)
#tree_preds <- predict(tree_house, test_set, type = "class")
#mean(test_set$Neighborhood != tree_preds)

0.37 * sd_year + mean_year
```