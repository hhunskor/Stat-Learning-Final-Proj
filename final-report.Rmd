---
title: "Math 218: Final Report"
subtitle: "Project Haydoja"
author: "Payoja and Hayden"
output: pdf_document
date: "Dec 6"
editor_options: 
chunk_output_type: inline
---

```{r echo=FALSE,message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo=FALSE,message=FALSE, warning=FALSE)
```

```{r Load Packages}
library('fastDummies')
library(tidyverse)
library(dplyr)
library(tidyr)
library(ggcorrplot)
library(xgboost)
library(tree)
library(glmnet)
library(e1071)
```

## Introduction

There is a plethora of statistical tools available to understand any given set of data. Data scientists and statisticians often use either supervised or unsupervised statistical learning methods to model their data to both better understand trends and extrapolate predictions. For the our project, however, we decided to exclusively focus on supervised learning, mainly for two reasons. First, supervised learning methods are always guided by a response variable which ensures our research objective is clearly defined. Second, supervised learning models have relevant performance metrics (i.e. RMSE for regression problems or misclass. rate for classification problems).

For this project, we decided to implement a variety of supervised learning techniques using the Ames Housing dataset, which is a publicly available dataset that includes extensive information on the sale of residential properties in Ames, Iowa from 2006 to 2010. It contains 1460 observations with 79 variables on various house specifications. The variables are a mix of continuous, categorical and discrete types.

The data is available on [Kaggle](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data).

We will investigate both regression and classification problems, with the primary goal of better understanding which statistical learning method perform best on different features from the Ames housing dataset. Below are our driving research questions:

1. Which statistical learning method most accurately predicts housing prices using other available housing features?

2. Which statistical learning method most accurately classifies houses by neighborhood based on the other available housing features?

Although our primary goal is to identify which statistical learning method performs best, we will also spend some time exploring and understanding which predictors affect our response variables and how.

### Data Description

```{r Load Data}
original_dat <- read_csv("data/train.csv")
```

Rather than use all 79 available variables, we decided to only include features we thought were particularly relevant to our two driving research questions. This dataset also includes a lot of categorical variables, so we decided to only include the ones that can be easily translated to usable dummy/binary variables for the sake of simplicity.

Below is a description of the features we focused on for this project:

##### Categorical Variables:

- Neighborhood: Physical locations within Ames city limits
- CentralAir: Central air conditioning (Yes/No)
- PavedDrive: Paved driveway (Yes/No)
- GarageFinish: Interior finish of the garage
- BldgType: Type of dwelling
- HouseStyle: Style of dwelling
- BsmtFinType1: Rating of basement finished area

##### Continuous Variables:

- SalePrice: Price of house 
- PoolArea: Pool area in square feet
- LotArea: Lot size in square feet
- TotalBsmtSF: Total square feet of basement area
- GarageArea: Size of garage in square feet
- GrLivArea: Above ground living area square feet

##### Discrete Variable

- FullBath: Number of full bathrooms above ground
- HalfBath: Number of half bathrooms above ground
- BedroomAbvGr: Number of bedrooms above ground
- KitchenAbvGr: Number of kitchens above ground
- Fireplaces: Number of fireplaces
- OverallQual: Rates the overall material and finish of the house
- OverallCond: Rates the overall condition of the house
- YearBuilt: Original construction date
- YearRemodAdd: Year house was remodeled
- YrSold: Year house was sold

One caveat to this trimmed feature list is that there is a feature 'SaleCondition' that defines the sale of a house as either "normal" or one of several types of abnormal sales. We only want to look at the normal sales, so we've selected only those observations with SaleCondition = "normal".

```{r Filtering Abnormal Sales}
original_dat <- filter(original_dat, SaleCondition == "Normal")
```

```{r Trim Features}
#The code chunk below further trims our original data to include just the features we are interested in:
dat <- original_dat %>%
  select(Neighborhood, CentralAir, PavedDrive, GarageFinish, BldgType, HouseStyle, BsmtFinType1,
         SalePrice, PoolArea, LotArea, TotalBsmtSF, GarageArea, GrLivArea, FullBath, HalfBath,
         BedroomAbvGr, KitchenAbvGr, Fireplaces, OverallQual, OverallCond, YearBuilt, YearRemodAdd, YrSold, PavedDrive)
```

## EDA

#### Investigating SalePrice Variable

The first exploratory question we wanted to investigate is if there is, in fact, a correlation between how big a house is and how much it costs:

```{r LotArea vs. SalePrice}
ggplot(dat, aes(x = LotArea, y = SalePrice)) +
  geom_point()
```

This plot suggests that, although there may be a slight positive correlation between the LotArea and SalePrice, there are likely other variables that contribute to how expensive a house is.

Let's now look a bit more at the SalePrice variable and its distribution:

```{r Sale Price EDA}
#Histogram of house prices
dat %>%
  ggplot(., aes(x = SalePrice)) +
  geom_histogram()

#Average house price over time
dat %>%
  group_by(YrSold) %>%
  summarise(meanPrice = mean(SalePrice)) %>%
  ggplot(., aes(x = YrSold, y = meanPrice)) +
  geom_point() + geom_line()
```

Judging from the histogram, the distribution of SalePrice in our data set is slightly right skewed. This suggests that there are some outlier properties with significantly higher sale prices. To account for this, we will add a logarithmic transformation to the SalePrice variable (see data cleaning section).

In the second plot, we can observe a huge drop in average sale price of houses in our data coinciding with the 2008 housing market crash. To try to account for this, we decided to replace YrSold with a binary variable 'Sold_08' that will be Y if a house was sold in 2008, and N otherwise:

```{r Accounting for Housing Crash}
dat$Sold_08 <- as.factor(ifelse(dat$YrSold == '2008', 'Y', 'N'))
dat <- select(dat, -YrSold)
```

#### Investigating Neighborhood Variable

We also wanted to look at the distribution of houses across neighborhoods, as well as how expensive the homes are in each of the neighborhoods:

```{r Distribution of Houses By Neighborhood}
dat %>%
  count(Neighborhood) %>%
  ggplot(aes(x = n, y = reorder(Neighborhood, n))) +
  geom_bar(stat = "identity") +
  labs(title = "Number of Houses in Each Neighborhood",
       x = "Number of Houses",
       y = "Neighborhood")
```

This plot reveals that there is a pretty big gradient of neighborhood sizes. To simplify our models and their interpretations, we want to reduce the number of neighborhood categories by grouping some of the neighborhoods into an 'Other' category.

To do so, we first isolated the neighborhoods with fewer than 50 homes:

```{r}
other_neighborhoods <- dat %>%
  group_by(Neighborhood) %>%
  filter(n() < 50) %>%
  distinct(Neighborhood)
```

We then looked at how these 16 neighborhoods vary in terms of average price and average house age:

```{r}
dat %>%
  filter(dat$Neighborhood %in% other_neighborhoods$Neighborhood) %>%
  group_by(Neighborhood) %>%
  summarise(avg_built_date = mean(YearBuilt),
            mean = mean(SalePrice)) %>%
  arrange(-avg_built_date) %>%
  ggplot(aes(x = avg_built_date, y = mean)) +
  geom_text(aes(label = Neighborhood))
```

Right off the bat, we decided to merge NoRidge (Northridge) and NridgHt (Northridge Heights) because they both are comprised primarily of new, expensive homes. Also, a quick Google search revealed that the two neighborhoods border each other geographically. We defined this combined neighborhood as 'GrNoRidge' or Greater Northridge:

```{r}
dat$Neighborhood <- ifelse(dat$Neighborhood %in% c('NoRidge', 'NridgHt'), 'GrNoRidge', dat$Neighborhood)
```

Other than that, there weren't really any other neighborhood merges that made sense geographically. And if we were to merge all the neighborhoods with < 50 houses, we would get an 'Other' neighborhood category that accounts for the second highest number of houses in the dataset. This would likely skew how our model predicts neighborhood classification.

So, we decided look at how much data we would be losing if we just dropped the neighborhoods with < 50 houses or < 30 houses:

```{r}
nrow(filter(dat, dat$Neighborhood %in% other_neighborhoods$Neighborhood))

#Just < 30 houses
other_neighborhoods <- dat %>%
  group_by(Neighborhood) %>%
  filter(n() < 30) %>%
  distinct(Neighborhood)

nrow(filter(dat, dat$Neighborhood %in% other_neighborhoods$Neighborhood))
```

We decided to go ahead and drop the 180 houses in neighborhoods with < 30 houses:

```{r}
dat <- dat %>%
  filter(! Neighborhood %in% other_neighborhoods$Neighborhood)
```

#### Correlation between some of the numeric and discrete data fields (most importantly SalePrice):

```{r Numeric Correlation}
corr_mtx <- cor(select_if(dat, is.numeric), use = "pairwise.complete.obs")

ggcorrplot(corr_mtx, # input the correlation matrix
           hc.order = TRUE, 
           type = "upper", 
           lab = TRUE,
           method = "circle", 
           colors = c("tomato2", "white", "springgreen3"))
```

As seen from above, housing prices seem to be the most correlated (positively) with overal quality of the house, above ground living area (in sq. ft), total basement area (in sq. ft) and garage area (in sq. ft). The price is also moderately correlated with number of full bathrooms.

## Data Cleaning and Scaling

#### Feature Engineering

There are a couple problems with the way our variables are formatted. First, if a house has not been remodeled, the YearRemodAdd will be the same as YearBuilt. Let's look at how many houses this case applies to:

```{r}
nrow(filter(dat, YearRemodAdd == YearBuilt))
```

Since over half the houses have never been remodeled, we just decided to convert YearRemodAdd to a binary variable:

```{r Remodel Variable}
dat$Remod <- as.factor(ifelse(dat$YearBuilt == dat$YearRemodAdd, 'Y', 'N'))
dat <- select(dat, -YearRemodAdd)
```

Additionally, PavedDrive is a categorical variable with three possible class: Y (if paved), P (if partially paved) and N (if dirt/gravel). For simplicity, we decided to recode the variable so that PavedDrive is a binary variable that takes Y(Yes) if it is paved and N (No) otherwise.

```{r PavedDrive Variable}
dat$PavedDrive <- ifelse((dat$PavedDrive == "Y"), "Y", "N")
```

#### Scaling/Transformations

Here is where we want to apply the log transformation to our SalePrice variable:

```{r SalePrice Log Transform}
dat$SalePrice <- log(dat$SalePrice)
```

Now, we want to scale all of our numeric variables:

```{r Scaling}
mean_year<-mean(dat$YearBuilt)
sd_year<-sd(dat$YearBuilt)

num_vars <- names(dat %>% select_if(is.numeric))
num_vars <- num_vars[! num_vars == "SalePrice"] #We didn't scale SalePrice because we already added a log transformation
dat[, num_vars] <- scale(dat[, num_vars], center=TRUE, scale=TRUE)
```

#### Handling NAs

One final issue with our data is that some features still have NA values:

```{r NA Values}
sapply(dat, function(x) sum(is.na(x)))
```

Both 'GarageFinish' and 'BsmtFinType1' are NA when a house doesn't have that respective feature. To handle this, we replaced those NA values with their own classification:

```{r Handle NAs}
dat$GarageFinish <- ifelse(is.na(dat$GarageFinish), "NoGar", dat$GarageFinish)
dat$BsmtFinType1 <- ifelse(is.na(dat$BsmtFinType1), "NoBsmt", dat$BsmtFinType1)
```

## Methodology

### Regression

To answer our first research question - "Which statistical learning method most accurately predicts housing prices using other available housing features?" - we decided to use five different regression techniques: simple linear regression, k-fold CV regression, ridge regression, lasso regression, and XGBoost.

For all models aside from K-fold CV regression, we divide our data into two sets 'train' and 'test'. The 'train' set contains 80% of our entire data where as the 'test' set contains the remaining 20%. The response variable is 'SalePrice' and the predictors are the other 22 features we included. 

Below is a brief description of each method:

**1. Simple Linear Regression**
We trained our simple linear regression model on test data to predict SalePrice using all available predictors. For categorical variables, we create dummy variables.To assess the performance of our model, we predicted the sale price of houses in the test set, and examined the test error rate by measuring the root mean squares error.

**2. K-fold CV Linear Regression**
To implement K-fold CV linear regression, we used the same modeling technique as simple linear regression, but this time using 10-fold cross validation. To assess model performance, we took the average of the test RMSE across all 10 folds.

**3. Ridge Regression**
We implemented the ridge regression to examine whether shrinking features that are not as important improves the performance of the model. Using the 22 predictors, we fit the ridge model on the training set, with $\lambda$ chosen by 10-fold cross-validation. For categorical variables, we create dummy variables. To validate the performance of our model, we predicted the sale price of houses in the test set and then examined the test error rate by measuring the root mean squares error. The R package required to implement this method is 'glmnet'.

**4. Lasso Regression**
22 predictors, although better than having too few predictors, may be too many to use for this research question. Thus, we want to implement the lasso method so that coefficients of features that are not as important shrinks to 0. Using the 22 predictors, we fit the lasso model on the training set, with $\lambda$ chosen by cross-validation with k-fold equals 10. For categorical variables, we create dummy variables. We realize that interpreting this might be tricky as only coefficient on some of the classes might be shrunk to 0, but we believe features such as neighborhood are important in determining the sale price.Thus, we chose to create dummies for categorical variables instead of getting rid of them. To validate the performance of our model, we predicted the sale price of houses in the test set using the lasso model fit on the training set. Then, examined the test error rate by measuring the root mean squares error. The R package required to implement this method is 'glmnet'.

**5. XGBoost**
The XGBoost algorithm stands for eXtreme Gradient Boosting and is one of the most popular gradient boosted trees algorithms. At a high level, the XGBoost algorithm works by building decision trees on the training data, and then combining the predictions made by these trees to make a final prediction. The decision trees are built in a way that tries to minimize the loss function, which is a measure of how far the predicted values are from the true values in the training data.

One key feature of the XGBoost algorithm is that it uses regularization to prevent overfitting. This is achieved by adding a regularization term to the loss function, which penalizes models that are too complex.

For our project, we trained the XGBoost algorithm on all available predictors - we had to convert categorical variables to dummy variables because XGBoost can't handle categorical data. To assess model performance, we predicted sale price of houses in the test set using the model and calculated RMSE. The R package required to implement this method is 'xgboost'.

### Classification

To answer our second research question relating to the classification problem, "Which statistical learning method most accurately classifies houses by neighborhood based on the other available housing features?", we decided to use two different classification models: Naive Bayes and Tree-Based Classification. For both methods we divide our data into two sets 'train' and 'test'. The 'train' set contains 80% of our entire data where as the 'test' set contains the remaining 20%. The train and the test set are the same for all both models.

Below is a brief description of each method:

**1. Naive Bayes**
To implement Naive Bayes, we used the naiveBayes() function, which is part of the e1071 library, on our test set. To evaluate the performance of our model, we predicted the response on the test data and produced a contingency table comparing the true test labels to the predictions.

**2. Tree-Based Classification**
To implement this method, we fit a decision classification tree to the training data, using 'Neighborhood' as the response variable. We used all available predictors to fit the model. To evaluate the performance of our model, we predictED the response on the test data and produceD a contingency table comparing the true test labels to the predictions.

## Results

### Which statistical learning method most accurately predicts housing prices using other available housing features?

#### Simple Linear Regression

```{r Creating Test and Train Sets}
set.seed(1)
n <- nrow(dat)

train_ids <- sample(1:n, .8 * n)
test_ids <- (1:n)[-train_ids]
```

```{r Basic Linear Regression}
#Fit model
mod1 <- lm(SalePrice ~ ., dat[train_ids,])

#Make predictions
mod1_preds <- predict(mod1, dat[test_ids,])

#RMSE
sqrt(mean((exp(dat$SalePrice[test_ids]) - exp(mod1_preds)) ^ 2))
coef(mod1)
```

Based on the output of the regression model, the 5 variables that seems to affect the sale price of the houses are 
GrLivArea, NeighborhoodCrawfor (a dummy that indicates whether house is in 'Crawfor' neighborhood ), OverallQual, YearBuilt and TotalBsmtSF.Since the numeric variables are scaled, it is a little tricky to estimate by how much these variables affect the Sale price of the house. However, the sign on the coefficients of these variables are informative in that it indicates in which direction the price of the house will change as these variable changes. For example, the coefficient of 0.114 on GrLivArea implies that, controlling for other factors, an increase in above ground living area(in sq ft) increases the sale price of the house. The coefficient of 0.0963 on NeighborhoodCrawfor implies that, controlling for other factors, the sale price of houses in Crawfor Neighborhood are higher than sale price of houses in BrkSide  Neighborhood (which is our baseline neighborhood).

The Root Mean Squared Error (RMSE), which has been re-scaled to USD, is 17498.74. 

This means that the prediction of houses in the test set is, on average, off by $17,498.74. This error seems small given that the sale prices of the houses are much higher.

#### CV Regression

```{r CV Linear Regression}
set.seed(1)
K <- 10

# randomly split the indices/observations into K groups of roughly equal size
rand <- sample(1:nrow(dat))
group_ids <- split(rand, cut(seq_along(rand), K, labels = FALSE))

rmse_vec <- rep(NA,K)
n <- nrow(dat)

for(i in 1:K){
  test_ids <- group_ids[[i]]
  train_ids <- (1:n)[-test_ids]
  
  train_mod <- lm(SalePrice ~ ., data = dat[train_ids, ])
  pred <- predict(train_mod, newdata = dat[test_ids,])
  mse_i <- (exp(dat$SalePrice[test_ids]) - exp(pred))^2
  rmse_vec[i] <- sqrt(mean(mse_i))
}

mean(rmse_vec)
```

The Root Mean Squared Error (RMSE), which has been re-scaled to USD, is 17950.85.

This means that the prediction of houses in the test set is,on average, off by $17,950.85. Again, this error seems relatively small.

#### Ridge Regression

```{r}
x <- model.matrix(SalePrice ~ ., dat)[,-1]
y <- dat$SalePrice
```

```{r}
y_test <- y[test_ids]
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 0, nfolds = 10)
best_lam <- cv_out$lambda.min

#find RMSE
grid <- 10^seq(5,-2,length = 100)
ridge_mod <- glmnet(x[train_ids,], y[train_ids], alpha = 0, lambda = grid)
ridge_preds <- predict(ridge_mod, s= best_lam, newx = x[test_ids,])

sqrt(mean((exp(ridge_preds) - exp(y_test))^2))

ridge_final <- glmnet(x, y, alpha = 0, lambda = best_lam)
round(coef(ridge_final), 3)
```

Based on the output of the regression model, the 5 variables that seems to affect the sale price of the houses are 
Neighborhood_Crawfor (a dummy that indicates whether house is in 'Crawfor' neighborhood), GrLivArea, NeighborhoodGrNoRidge
(a dummy that indicates whether house is in 'GrNoRidge' neighborhood), OverallQual and NeighborhoodSomerst (a dummy that indicates whether house is in 'Somerst' neighborhood). The interpretation is similar to that of linear regression.

The RMSE error, which has been re-scaled to USD, is 16196.87.

This implies that the prediction of houses in the test set is, on average, off by $16,196.87. This is the smallest test RMSE we've seen so far.

#### Lasso Regression

```{r}
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 1, nfolds = 10)
best_lam <- cv_out$lambda.min
grid <- 10^seq(5,-2,length = 100)

#coefficients
lasso_mod <- glmnet(x[train_ids,], y[train_ids], alpha = 1, lambda = grid)
lasso_coef <- predict(lasso_mod, type = "coefficients", s = best_lam)#[1:18,]
lasso_coef

#RMSE
sqrt(mean((exp(predict(lasso_mod, s= best_lam, newx = x[test_ids,])) - exp(y_test))^2))
```

Based on the output, the lasso model scaled the coefficients on 28 of the 49 variables to 0. The interpretation of the coefficients of some of the categories in Neighborhoods, Building Type, HouseStyle, Basement Finish Type and Garage finish getting scaled to 0 is a bit trickier  since these variables were categorical variables with multiple classes but only coefficients of certain classes were scaled to 0. It is hard to tell whether this implies these variables are not important covariates or only certain classes of these categorical variables are important when predicting for saleprice.

The 5 variables that seems to affect the sale price of the houses are GrLivArea, YearBuilt, NeighborhoodCrawfor  (a dummy that indicates whether house is in 'Crawfor' neighborhood), OverallQual and CentralAirY(a dummy that indicates whether house is in 'Crawfor' neighborhood). The interpretation for GrLivArea, NeighborhoodCrawfor  and  OverallQual is similar to that of Linear Regression. The coefficient of  0.08 on YearBuilt implies that, controlling for other variables, newwly built houses are more expensive. Likewise, the coefficient on CentralAirY implies that, controlling for other variables, houses that have central air conditioning have sale prices higher than houses that don't have central air conditioning.

The RMSE error, which has been re-scaled to USD, is 17882.54. This implies that the prediction of houses in the test set is, on average, off by $17,882.54.

#### XGBoost

We chose to include the code chunk where we prepare the XGBoost model data just because this is a method we haven't looked at in this class.

```{r echo = T}
#Create dataset that XGBoost can handle
xg_dat <- dummy_cols(dat, remove_first_dummy = TRUE)

#Drop old categorical columns
cat_name <- names(xg_dat)[-which(sapply(xg_dat, is.numeric))]
xg_dat <- xg_dat[,!(names(xg_dat) %in% cat_name)]

#Replace numeric NAs with 0s.
xg_dat[is.na(xg_dat)] = 0

#Create test and train sets
train_x <- data.matrix(select(xg_dat[train_ids,], -SalePrice))
train_y<-xg_dat[train_ids,]$SalePrice

test_x<-data.matrix(select(xg_dat[test_ids,], -SalePrice))
test_y<-xg_dat[test_ids,]$SalePrice

xgb_train<-xgb.DMatrix(data = train_x, label = train_y)
xgb_test<-xgb.DMatrix(data = test_x, label = test_y)
```

Note: an interesting feature of XGBoost is tuning different hyperparameters. We did not have time to fully investigate/learn how these parameters work, so we ignored most of them.

```{r include = F}
#Create model
xgb_mod <- xgboost(data = xgb_train, max.depth = 2, nrounds = 100) #We arbitrarily chose to run this for 100 rounds
```

If we look at the model at each iteration, the train RMSE decreases slightly. It is hard to understand what exactly the train RMSE values mean just because they are calculated based on the log-transformed SalePrice value.

However, we can convert the test RMSE values back to USD values using the exp() function:

```{r}
pred_y = predict(xgb_mod, xgb_test)
sqrt(mean((exp(test_y) - exp(pred_y))^2))
```

This means that the prediction of houses in the test set is, on average, off by $18,810.44.

### Which  statistical learning method most accurately classifies houses by neighborhood based on the other available housing features?

#### Naive Bayes
```{r}
# Fitting Naive Bayes Model

classifier_cl <- naiveBayes(Neighborhood ~ ., dat[train_ids,])
#classifier_cl
 
# Predicting on test data
y_pred <- predict(classifier_cl, newdata = dat[test_ids,])
 
# Confusion Matrix
cm <- table(dat[test_ids,]$Neighborhood, y_pred)
 
# Model Evaluation
mean(dat[test_ids,]$Neighborhood != y_pred)
cm
```

The misclassification rate is ~0.56, which is really high.

#### Tree

```{r}
tree_dat <-dat %>%
  mutate_if(is.character, factor)
tree_house<-tree(as.factor(Neighborhood) ~ .,tree_dat[train_ids,])
summary(tree_house)
plot(tree_house)
text(tree_house,pretty=0,cex=.5)
```

Based on the tree, it seems like YearBuilt is the most important factor for determining the class of the species. Since the data is scaled, it is a little tricky to interpret the tree but we we wanted to we could un-scale the data to know the exact value the split was the based on. For example, the root of the tree, YearBuilt < 0.07, can be un-scaled so that the root note is: 

```{r}
0.07 * sd_year + mean_year
```

The contingency table for the test set is:
```{r}
tree_preds <- predict(tree_house, tree_dat[test_ids,], type = "class")
table(preds = tree_preds, true = tree_dat[test_ids,]$Neighborhood)
```

The misclassification rate is:

```{r}
mean(tree_dat[test_ids,]$Neighborhood != tree_preds)
```

The misclassification rate of ~ 0.45 is still quite high. So we decided to check whether pruning the tree will improve the results.

```{r}
cv_house <- cv.tree(tree_house, FUN = prune.misclass, K = 10)

data.frame(error_rate = cv_house$dev, size = cv_house$size, k = cv_house$k) %>%
  pivot_longer(cols = 2:3, names_to = "var", values_to = "value") %>%
  ggplot(., aes(x = value, y = error_rate)) +
  geom_point()+
  geom_line()+
  facet_wrap(~var, scales = "free")
```
Based on the output, the tree with 13 terminal nodes seems to be the best. Thus, we will prune the original tree to obtain the 13 node tree.  
```{r}
prune_house<- prune.misclass(tree_house, best = 13)
plot(prune_house)
text(prune_house,pretty=0,cex=.5)
```

The misclassification rate on the test set is then:

```{r}
prune_pred <- predict(prune_house, tree_dat[test_ids,],
                     type = "class")
mean(tree_dat[test_ids,]$Neighborhood != prune_pred)
```

The misclassification rate of the pruned tree did not change significantly. This makes sense because pruning only reduced the number of terminal nodes by 1--the original tree had 14 terminal nodes whereas the pruned one has 13.

## Discussion

To summarize the results of our regression models, we found that the Root Mean Squared Error (RMSE) was 17498.74 for simple linear regression model, 17950.85 for linear regression with cross-validation, 16196.87 for ridge regression model and 17882.54 for the lasso model and 18810.44 for the XGBoost method. Thus, our best performing model was the ridge regression. This indicates that most of the predictors that were used in the regression impact the sale price significantly. Overall, we think that the our best performing model is a good choice to predict the sale prices as RMSE of 16196.87 is low if we compare it to the sale price of houses, which are much higher.

To summarize the results of our classification models, we found the misclassification rates of the Naive Bayes and Decision Tree models to be approximately .67 and .45, respectively. Although the tree-based method performs significantly better, we still think that the misclassification rate is really high. This suggests that perhaps the neighborhoods are not clustered based solely on the predictors we used.

## Limitations

There are several big limitations to this project that we struggled to address. First, this dataset has so many variables to use that it was challenging to select the most important ones. We tried using Ridge and Lasso to handle this issue, but it got pretty messy with all the categorical variables.

Second, we arbitrarily set a seed and used the same train/test split for all our regression models (aside from the k-fold CV linear regression). Because we were only working with just over 1000 observations, the way we split our data could have a huge impact on the resulting RMSE. For example, we tried setting several different seeds and re-running the models and each time we got significantly different error values. The only model which was relatively consistent was the k-fold CV regression, which makes intuitive sense. If we had more time, we would've liked to investigate and implement other re-sampling techniques, specifically the CV XGBoost model.

The third main limitation with this project was handling the Neighborhood variable for classification. In our EDA section, we showed that there are a lot of neighborhoods that have very few houses. This makes it pretty difficult to create a model that will accurately predict those specific neighborhoods for a given test observation. We tried to account for this issue by dropping the neighborhoods with < 30 houses, but that is an imperfect solution and further limits the amount of data we had to train our models on.

The fourth main limitation with this project was interpreting coefficients with categorical variables that had been converted to dummy variables. Our dataset had categorical features that have many different values which in turn created a lot of dummy variables in our models. This made it challenging to identify which of the classifications for a given variable were actually statistically significant.
