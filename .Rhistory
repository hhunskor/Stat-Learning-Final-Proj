sqrt(mean((exp(ridge_preds) - exp(y_test))^2))
ridge_final <- glmnet(x, y, alpha = 0, lambda = best_lam)
round(coef(ridge_final), 3)
y_test<-y[test_ids]
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 0, nfolds = 10)
best_lam <- cv_out$lambda.min
#find RMSE
grid <- 10^seq(5,-2,length = 100)
ridge_mod <- glmnet(x[train_ids,], y[train_ids], alpha = 0, lambda = grid)
ridge_preds <- predict(ridge_mod, s= best_lam, newx = x[test_ids,])
sqrt(mean((exp(ridge_preds) - exp(y_test))^2))
ridge_final <- glmnet(x, y, alpha = 0, lambda = best_lam)
round(coef(ridge_final), 3)
library(glmnet)
x <- model.matrix(SalePrice ~ ., dat)[,-1]
y<-dat$SalePrice
n <- nrow(dat)
set.seed(1)
n <- nrow(dat)
train_ids <- sample(1:n, .80*n)
test_ids <- (1:n)[-train_ids]
mod1 <- lm(SalePrice ~ ., dat[train_ids,])
#Make predictions
mod1_preds <- predict(mod1, dat[test_ids,])
#RMSE
sqrt(mean((exp(dat$SalePrice[test_ids]) - exp(mod1_preds)) ^ 2))
coef(mod1)
set.seed(1)
K <- 10
# randomly split the indices/observations into K groups of roughly equal size
rand <- sample(1:nrow(dat))
group_ids <- split(rand, cut(seq_along(rand), K, labels = FALSE))
rmse_vec <- rep(NA,K)
n <- nrow(dat)
for(i in 1:K){
test_ids <- group_ids[[i]]
train_ids <- (1:n)[-test_ids]
train_mod <- lm(SalePrice ~ ., data = dat[train_ids, ])
pred <- predict(train_mod, newdata = dat[test_ids,])
mse_i <- (exp(dat$SalePrice[test_ids]) - exp(pred))^2
rmse_vec[i] <- sqrt(mean(mse_i))
}
mean(rmse_vec)
library(glmnet)
x <- model.matrix(SalePrice ~ ., dat)[,-1]
y<-dat$SalePrice
n <- nrow(dat)
y_test<-y[test_ids]
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 0, nfolds = 10)
best_lam <- cv_out$lambda.min
#find RMSE
grid <- 10^seq(5,-2,length = 100)
ridge_mod <- glmnet(x[train_ids,], y[train_ids], alpha = 0, lambda = grid)
ridge_preds <- predict(ridge_mod, s= best_lam, newx = x[test_ids,])
sqrt(mean((exp(ridge_preds) - exp(y_test))^2))
ridge_final <- glmnet(x, y, alpha = 0, lambda = best_lam)
round(coef(ridge_final), 3)
library(glmnet)
x <- model.matrix(SalePrice ~ ., dat)[,-1]
y<-dat$SalePrice
n <- nrow(dat)
y_test<-y[test_ids]
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 0, nfolds = 10)
best_lam <- cv_out$lambda.min
#find RMSE
grid <- 10^seq(5,-2,length = 100)
ridge_mod <- glmnet(x[train_ids,], y[train_ids], alpha = 0, lambda = grid)
ridge_preds <- predict(ridge_mod, s= best_lam, newx = x[test_ids,])
sqrt(mean((exp(ridge_preds) - exp(y_test))^2))
ridge_final <- glmnet(x, y, alpha = 0, lambda = best_lam)
round(coef(ridge_final), 3)
library(glmnet)
x <- model.matrix(SalePrice ~ ., dat)[,-1]
y<-dat$SalePrice
n <- nrow(dat)
y_test<-y[test_ids]
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 0, nfolds = 10)
best_lam <- cv_out$lambda.min
#find RMSE
grid <- 10^seq(5,-2,length = 100)
ridge_mod <- glmnet(x[train_ids,], y[train_ids], alpha = 0, lambda = grid)
ridge_preds <- predict(ridge_mod, s= best_lam, newx = x[test_ids,])
sqrt(mean((exp(ridge_preds) - exp(y_test))^2))
ridge_final <- glmnet(x, y, alpha = 0, lambda = best_lam)
round(coef(ridge_final), 3)
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 1, nfolds = 10)
best_lam <- cv_out$lambda.min
grid <- 10^seq(5,-2,length = 100)
#coefficients
lasso_mod <- glmnet(x[train_ids,], y[train_ids],alpha = 1, lambda = grid)
lasso_coef <- predict(lasso_mod, type = "coefficients", s = best_lam)#[1:18,]
lasso_coef
#RMSE
sqrt(mean((exp(predict(lasso_mod, s= best_lam, newx = x[test_ids,])) - exp(y_test))^2))
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 1, nfolds = 10)
best_lam <- cv_out$lambda.min
grid <- 10^seq(5,-2,length = 100)
#coefficients
lasso_mod <- glmnet(x[train_ids,], y[train_ids],alpha = 1, lambda = grid)
lasso_coef <- predict(lasso_mod, type = "coefficients", s = best_lam)#[1:18,]
lasso_coef
#RMSE
sqrt(mean((exp(predict(lasso_mod, s= best_lam, newx = x[test_ids,])) - exp(y_test))^2))
set.seed(3)
n <- nrow(dat)
#Create vector of indices for each set
train_ids <- sample(1:n, .80*n)
test_ids <- (1:n)[-train_ids]
#Create dataset that XGBoost can handle
xg_dat <- dummy_cols(dat, remove_first_dummy = TRUE)
#Drop old categorical columns
cat_name <- names(xg_dat)[-which(sapply(xg_dat, is.numeric))]
xg_dat <- xg_dat[,!(names(xg_dat) %in% cat_name)]
#Replace numeric NAs with 0s.
xg_dat[is.na(xg_dat)] = 0
#Create test and train sets
train_x <- data.matrix(select(xg_dat[train_ids,], -SalePrice))
train_y<-xg_dat[train_ids,]$SalePrice
test_x<-data.matrix(select(xg_dat[test_ids,], -SalePrice))
test_y<-xg_dat[test_ids,]$SalePrice
xgb_train<-xgb.DMatrix(data = train_x, label = train_y)
xgb_test<-xgb.DMatrix(data = test_x, label = test_y)
#Create model
xgbc<-xgboost(data = xgb_train, max.depth = 2, nrounds = 100) #We arbitrarily chose to run this for 100 rounds
#Note: it is hard to understand what exactly the train RMSE values here mean just because they are calculated based on the log-transformed SalePrice value.
pred_y = predict(xgbc, xgb_test)
sqrt(mean((exp(test_y) - exp(pred_y))^2))
set.seed(1)
y_test<-y[test_ids]
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 0, nfolds = 10)
best_lam <- cv_out$lambda.min
#find RMSE
grid <- 10^seq(5,-2,length = 100)
ridge_mod <- glmnet(x[train_ids,], y[train_ids], alpha = 0, lambda = grid)
ridge_preds <- predict(ridge_mod, s= best_lam, newx = x[test_ids,])
sqrt(mean((exp(ridge_preds) - exp(y_test))^2))
ridge_final <- glmnet(x, y, alpha = 0, lambda = best_lam)
round(coef(ridge_final), 3)
set.seed(1)
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 1, nfolds = 10)
best_lam <- cv_out$lambda.min
grid <- 10^seq(5,-2,length = 100)
#coefficients
lasso_mod <- glmnet(x[train_ids,], y[train_ids],alpha = 1, lambda = grid)
lasso_coef <- predict(lasso_mod, type = "coefficients", s = best_lam)#[1:18,]
lasso_coef
#RMSE
sqrt(mean((exp(predict(lasso_mod, s= best_lam, newx = x[test_ids,])) - exp(y_test))^2))
set.seed(1)
n <- nrow(dat)
#Create vector of indices for each set
train_ids <- sample(1:n, .80*n)
test_ids <- (1:n)[-train_ids]
#Create dataset that XGBoost can handle
xg_dat <- dummy_cols(dat, remove_first_dummy = TRUE)
#Drop old categorical columns
cat_name <- names(xg_dat)[-which(sapply(xg_dat, is.numeric))]
xg_dat <- xg_dat[,!(names(xg_dat) %in% cat_name)]
#Replace numeric NAs with 0s.
xg_dat[is.na(xg_dat)] = 0
#Create test and train sets
train_x <- data.matrix(select(xg_dat[train_ids,], -SalePrice))
train_y<-xg_dat[train_ids,]$SalePrice
test_x<-data.matrix(select(xg_dat[test_ids,], -SalePrice))
test_y<-xg_dat[test_ids,]$SalePrice
xgb_train<-xgb.DMatrix(data = train_x, label = train_y)
xgb_test<-xgb.DMatrix(data = test_x, label = test_y)
#Create model
xgbc<-xgboost(data = xgb_train, max.depth = 2, nrounds = 100) #We arbitrarily chose to run this for 100 rounds
#Note: it is hard to understand what exactly the train RMSE values here mean just because they are calculated based on the log-transformed SalePrice value.
pred_y = predict(xgbc, xgb_test)
sqrt(mean((exp(test_y) - exp(pred_y))^2))
set.seed(3)
n <- nrow(dat)
#Create vector of indices for each set
train_ids <- sample(1:n, .80*n)
test_ids <- (1:n)[-train_ids]
#Create dataset that XGBoost can handle
xg_dat <- dummy_cols(dat, remove_first_dummy = TRUE)
#Drop old categorical columns
cat_name <- names(xg_dat)[-which(sapply(xg_dat, is.numeric))]
xg_dat <- xg_dat[,!(names(xg_dat) %in% cat_name)]
#Replace numeric NAs with 0s.
xg_dat[is.na(xg_dat)] = 0
#Create test and train sets
train_x <- data.matrix(select(xg_dat[train_ids,], -SalePrice))
train_y<-xg_dat[train_ids,]$SalePrice
test_x<-data.matrix(select(xg_dat[test_ids,], -SalePrice))
test_y<-xg_dat[test_ids,]$SalePrice
xgb_train<-xgb.DMatrix(data = train_x, label = train_y)
xgb_test<-xgb.DMatrix(data = test_x, label = test_y)
#Create model
xgbc<-xgboost(data = xgb_train, max.depth = 2, nrounds = 100) #We arbitrarily chose to run this for 100 rounds
#Note: it is hard to understand what exactly the train RMSE values here mean just because they are calculated based on the log-transformed SalePrice value.
pred_y = predict(xgbc, xgb_test)
sqrt(mean((exp(test_y) - exp(pred_y))^2))
set.seed(4)
n <- nrow(dat)
#Create vector of indices for each set
train_ids <- sample(1:n, .80*n)
test_ids <- (1:n)[-train_ids]
#Create dataset that XGBoost can handle
xg_dat <- dummy_cols(dat, remove_first_dummy = TRUE)
#Drop old categorical columns
cat_name <- names(xg_dat)[-which(sapply(xg_dat, is.numeric))]
xg_dat <- xg_dat[,!(names(xg_dat) %in% cat_name)]
#Replace numeric NAs with 0s.
xg_dat[is.na(xg_dat)] = 0
#Create test and train sets
train_x <- data.matrix(select(xg_dat[train_ids,], -SalePrice))
train_y<-xg_dat[train_ids,]$SalePrice
test_x<-data.matrix(select(xg_dat[test_ids,], -SalePrice))
test_y<-xg_dat[test_ids,]$SalePrice
xgb_train<-xgb.DMatrix(data = train_x, label = train_y)
xgb_test<-xgb.DMatrix(data = test_x, label = test_y)
#Create model
xgbc<-xgboost(data = xgb_train, max.depth = 2, nrounds = 100) #We arbitrarily chose to run this for 100 rounds
#Note: it is hard to understand what exactly the train RMSE values here mean just because they are calculated based on the log-transformed SalePrice value.
set.seed(5)
n <- nrow(dat)
#Create vector of indices for each set
train_ids <- sample(1:n, .80*n)
test_ids <- (1:n)[-train_ids]
#Create dataset that XGBoost can handle
xg_dat <- dummy_cols(dat, remove_first_dummy = TRUE)
#Drop old categorical columns
cat_name <- names(xg_dat)[-which(sapply(xg_dat, is.numeric))]
xg_dat <- xg_dat[,!(names(xg_dat) %in% cat_name)]
#Replace numeric NAs with 0s.
xg_dat[is.na(xg_dat)] = 0
#Create test and train sets
train_x <- data.matrix(select(xg_dat[train_ids,], -SalePrice))
train_y<-xg_dat[train_ids,]$SalePrice
test_x<-data.matrix(select(xg_dat[test_ids,], -SalePrice))
test_y<-xg_dat[test_ids,]$SalePrice
xgb_train<-xgb.DMatrix(data = train_x, label = train_y)
xgb_test<-xgb.DMatrix(data = test_x, label = test_y)
#Create model
xgbc<-xgboost(data = xgb_train, max.depth = 2, nrounds = 100) #We arbitrarily chose to run this for 100 rounds
#Note: it is hard to understand what exactly the train RMSE values here mean just because they are calculated based on the log-transformed SalePrice value.
pred_y = predict(xgbc, xgb_test)
sqrt(mean((exp(test_y) - exp(pred_y))^2))
set.seed(1)
n <- nrow(dat)
#Create vector of indices for each set
train_ids <- sample(1:n, .80*n)
test_ids <- (1:n)[-train_ids]
#Create dataset that XGBoost can handle
xg_dat <- dummy_cols(dat, remove_first_dummy = TRUE)
#Drop old categorical columns
cat_name <- names(xg_dat)[-which(sapply(xg_dat, is.numeric))]
xg_dat <- xg_dat[,!(names(xg_dat) %in% cat_name)]
#Replace numeric NAs with 0s.
xg_dat[is.na(xg_dat)] = 0
#Create test and train sets
train_x <- data.matrix(select(xg_dat[train_ids,], -SalePrice))
train_y<-xg_dat[train_ids,]$SalePrice
test_x<-data.matrix(select(xg_dat[test_ids,], -SalePrice))
test_y<-xg_dat[test_ids,]$SalePrice
xgb_train<-xgb.DMatrix(data = train_x, label = train_y)
xgb_test<-xgb.DMatrix(data = test_x, label = test_y)
#Create model
xgbc<-xgboost(data = xgb_train, max.depth = 2, nrounds = 100) #We arbitrarily chose to run this for 100 rounds
#Note: it is hard to understand what exactly the train RMSE values here mean just because they are calculated based on the log-transformed SalePrice value.
pred_y = predict(xgbc, xgb_test)
sqrt(mean((exp(test_y) - exp(pred_y))^2))
set.seed(3)
n <- nrow(dat)
#Create vector of indices for each set
train_ids <- sample(1:n, .80*n)
test_ids <- (1:n)[-train_ids]
#Create dataset that XGBoost can handle
xg_dat <- dummy_cols(dat, remove_first_dummy = TRUE)
#Drop old categorical columns
cat_name <- names(xg_dat)[-which(sapply(xg_dat, is.numeric))]
xg_dat <- xg_dat[,!(names(xg_dat) %in% cat_name)]
#Replace numeric NAs with 0s.
xg_dat[is.na(xg_dat)] = 0
#Create test and train sets
train_x <- data.matrix(select(xg_dat[train_ids,], -SalePrice))
train_y<-xg_dat[train_ids,]$SalePrice
test_x<-data.matrix(select(xg_dat[test_ids,], -SalePrice))
test_y<-xg_dat[test_ids,]$SalePrice
xgb_train<-xgb.DMatrix(data = train_x, label = train_y)
xgb_test<-xgb.DMatrix(data = test_x, label = test_y)
#Create model
xgbc<-xgboost(data = xgb_train, max.depth = 2, nrounds = 100) #We arbitrarily chose to run this for 100 rounds
#Note: it is hard to understand what exactly the train RMSE values here mean just because they are calculated based on the log-transformed SalePrice value.
pred_y = predict(xgbc, xgb_test)
sqrt(mean((exp(test_y) - exp(pred_y))^2))
set.seed(3)
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 1, nfolds = 10)
best_lam <- cv_out$lambda.min
grid <- 10^seq(5,-2,length = 100)
#coefficients
lasso_mod <- glmnet(x[train_ids,], y[train_ids],alpha = 1, lambda = grid)
lasso_coef <- predict(lasso_mod, type = "coefficients", s = best_lam)#[1:18,]
lasso_coef
#RMSE
sqrt(mean((exp(predict(lasso_mod, s= best_lam, newx = x[test_ids,])) - exp(y_test))^2))
set.seed(3)
y_test<-y[test_ids]
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 0, nfolds = 10)
best_lam <- cv_out$lambda.min
#find RMSE
grid <- 10^seq(5,-2,length = 100)
ridge_mod <- glmnet(x[train_ids,], y[train_ids], alpha = 0, lambda = grid)
ridge_preds <- predict(ridge_mod, s= best_lam, newx = x[test_ids,])
sqrt(mean((exp(ridge_preds) - exp(y_test))^2))
ridge_final <- glmnet(x, y, alpha = 0, lambda = best_lam)
round(coef(ridge_final), 3)
set.seed(3)
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 1, nfolds = 10)
best_lam <- cv_out$lambda.min
grid <- 10^seq(5,-2,length = 100)
#coefficients
lasso_mod <- glmnet(x[train_ids,], y[train_ids],alpha = 1, lambda = grid)
lasso_coef <- predict(lasso_mod, type = "coefficients", s = best_lam)#[1:18,]
lasso_coef
#RMSE
sqrt(mean((exp(predict(lasso_mod, s= best_lam, newx = x[test_ids,])) - exp(y_test))^2))
set.seed(3)
n <- nrow(dat)
#Create vector of indices for each set
train_ids <- sample(1:n, .80*n)
test_ids <- (1:n)[-train_ids]
#Create dataset that XGBoost can handle
xg_dat <- dummy_cols(dat, remove_first_dummy = TRUE)
#Drop old categorical columns
cat_name <- names(xg_dat)[-which(sapply(xg_dat, is.numeric))]
xg_dat <- xg_dat[,!(names(xg_dat) %in% cat_name)]
#Replace numeric NAs with 0s.
xg_dat[is.na(xg_dat)] = 0
#Create test and train sets
train_x <- data.matrix(select(xg_dat[train_ids,], -SalePrice))
train_y<-xg_dat[train_ids,]$SalePrice
test_x<-data.matrix(select(xg_dat[test_ids,], -SalePrice))
test_y<-xg_dat[test_ids,]$SalePrice
xgb_train<-xgb.DMatrix(data = train_x, label = train_y)
xgb_test<-xgb.DMatrix(data = test_x, label = test_y)
#Create model
xgbc<-xgboost(data = xgb_train, max.depth = 2, nrounds = 100) #We arbitrarily chose to run this for 100 rounds
#Note: it is hard to understand what exactly the train RMSE values here mean just because they are calculated based on the log-transformed SalePrice value.
library('fastDummies')
library(tidyverse)
library(dplyr)
library(tidyr)
library(ggcorrplot)
knitr::opts_chunk$set(echo=FALSE)
original_dat <- read_csv("data/train.csv")
original_dat <- filter(original_dat, SaleCondition == "Normal")
#The code chunk below further trims our original data to include just the features we are interested in:
dat <- original_dat %>%
select(Neighborhood, CentralAir, PavedDrive, GarageFinish, BldgType, HouseStyle, BsmtFinType1,
SalePrice, PoolArea, LotArea, TotalBsmtSF, GarageArea, GrLivArea, FullBath, HalfBath,
BedroomAbvGr, KitchenAbvGr, Fireplaces, OverallQual, OverallCond, YearBuilt, YearRemodAdd, YrSold, PavedDrive)
ggplot(dat, aes(x = LotArea, y = SalePrice)) +
geom_point()
#Histogram of house prices
dat %>%
ggplot(., aes(x = SalePrice)) +
geom_histogram()
#Average house price over time
dat %>%
group_by(YrSold) %>%
summarise(meanPrice = mean(SalePrice)) %>%
ggplot(., aes(x = YrSold, y = meanPrice)) +
geom_point() + geom_line()
dat$Sold_08 <- as.factor(ifelse(dat$YrSold == '2008', 'Y', 'N'))
dat <- select(dat, -YrSold)
dat %>%
count(Neighborhood) %>%
ggplot(aes(x = n, y = reorder(Neighborhood, n))) +
geom_bar(stat = "identity") +
labs(title = "Number of Houses in Each Neighborhood",
x = "Number of Houses",
y = "Neighborhood")
other_neighborhoods <- dat %>%
group_by(Neighborhood) %>%
filter(n() < 50) %>%
distinct(Neighborhood)
dat %>%
filter(dat$Neighborhood %in% other_neighborhoods$Neighborhood) %>%
group_by(Neighborhood) %>%
summarise(avg_built_date = mean(YearBuilt),
mean = mean(SalePrice)) %>%
arrange(-avg_built_date) %>%
ggplot(aes(x = avg_built_date, y = mean)) +
geom_text(aes(label = Neighborhood))
dat$Neighborhood <- ifelse(dat$Neighborhood %in% c('NoRidge', 'NridgHt'), 'GrNoRidge', dat$Neighborhood)
nrow(filter(dat, dat$Neighborhood %in% other_neighborhoods$Neighborhood))
other_neighborhoods <- dat %>%
group_by(Neighborhood) %>%
filter(n() < 30) %>%
distinct(Neighborhood)
nrow(filter(dat, dat$Neighborhood %in% other_neighborhoods$Neighborhood))
dat <- dat %>%
filter(! Neighborhood %in% other_neighborhoods$Neighborhood)
correlation <- dat %>%
select(SalePrice, PoolArea, LotArea, TotalBsmtSF, GarageArea, GrLivArea, FullBath, HalfBath,
BedroomAbvGr, KitchenAbvGr, Fireplaces, YearBuilt, OverallQual, OverallCond)
corr_mtx <- cor(correlation, use = "pairwise.complete.obs")
ggcorrplot(corr_mtx, # input the correlation matrix
hc.order = TRUE,
type = "upper",
lab = TRUE,
method = "circle",
colors = c("tomato2", "white", "springgreen3"))
nrow(filter(dat, YearRemodAdd == YearBuilt))
dat$Remod <- as.factor(ifelse(dat$YearBuilt == dat$YearRemodAdd, 'Y', 'N'))
dat <- select(dat, -YearRemodAdd)
dat$PavedDrive <- ifelse((dat$PavedDrive == "Y"), "Y", "N")
dat$SalePrice <- log(dat$SalePrice)
mean_year<-mean(tree_dat$YearBuilt)
sd_year<-sd(tree_dat$YearBuilt)
#First save mean and sd of SalePrice for un-scaling
#SP_mean <- mean(dat$SalePrice)
#SP_std <- sd(dat$SalePrice)
num_vars <- names(dat %>% select_if(is.numeric))
num_vars <- num_vars[! num_vars == "SalePrice"]
dat[, num_vars] <- scale(dat[, num_vars], center=TRUE, scale=TRUE)
sapply(dat, function(x) sum(is.na(x)))
dat$GarageFinish <- ifelse(is.na(dat$GarageFinish), "NoGar", dat$GarageFinish)
dat$BsmtFinType1 <- ifelse(is.na(dat$BsmtFinType1), "NoBsmt", dat$BsmtFinType1)
set.seed(1)
n <- nrow(dat)
train_ids <- sample(1:n, .80*n)
test_ids <- (1:n)[-train_ids]
mod1 <- lm(SalePrice ~ ., dat[train_ids,])
#Make predictions
mod1_preds <- predict(mod1, dat[test_ids,])
#RMSE
sqrt(mean((exp(dat$SalePrice[test_ids]) - exp(mod1_preds)) ^ 2))
coef(mod1)
set.seed(1)
K <- 10
# randomly split the indices/observations into K groups of roughly equal size
rand <- sample(1:nrow(dat))
group_ids <- split(rand, cut(seq_along(rand), K, labels = FALSE))
rmse_vec <- rep(NA,K)
n <- nrow(dat)
for(i in 1:K){
test_ids <- group_ids[[i]]
train_ids <- (1:n)[-test_ids]
train_mod <- lm(SalePrice ~ ., data = dat[train_ids, ])
pred <- predict(train_mod, newdata = dat[test_ids,])
mse_i <- (exp(dat$SalePrice[test_ids]) - exp(pred))^2
rmse_vec[i] <- sqrt(mean(mse_i))
}
mean(rmse_vec)
library(glmnet)
x <- model.matrix(SalePrice ~ ., dat)[,-1]
y<-dat$SalePrice
n <- nrow(dat)
set.seed(3)
y_test<-y[test_ids]
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 0, nfolds = 10)
best_lam <- cv_out$lambda.min
#find RMSE
grid <- 10^seq(5,-2,length = 100)
ridge_mod <- glmnet(x[train_ids,], y[train_ids], alpha = 0, lambda = grid)
ridge_preds <- predict(ridge_mod, s= best_lam, newx = x[test_ids,])
sqrt(mean((exp(ridge_preds) - exp(y_test))^2))
ridge_final <- glmnet(x, y, alpha = 0, lambda = best_lam)
round(coef(ridge_final), 3)
set.seed(3)
cv_out <- cv.glmnet(x[train_ids,], y[train_ids], alpha = 1, nfolds = 10)
best_lam <- cv_out$lambda.min
grid <- 10^seq(5,-2,length = 100)
#coefficients
lasso_mod <- glmnet(x[train_ids,], y[train_ids],alpha = 1, lambda = grid)
lasso_coef <- predict(lasso_mod, type = "coefficients", s = best_lam)#[1:18,]
lasso_coef
#RMSE
sqrt(mean((exp(predict(lasso_mod, s= best_lam, newx = x[test_ids,])) - exp(y_test))^2))
set.seed(3)
n <- nrow(dat)
#Create vector of indices for each set
train_ids <- sample(1:n, .80*n)
test_ids <- (1:n)[-train_ids]
#Create dataset that XGBoost can handle
xg_dat <- dummy_cols(dat, remove_first_dummy = TRUE)
#Drop old categorical columns
cat_name <- names(xg_dat)[-which(sapply(xg_dat, is.numeric))]
xg_dat <- xg_dat[,!(names(xg_dat) %in% cat_name)]
#Replace numeric NAs with 0s.
xg_dat[is.na(xg_dat)] = 0
#Create test and train sets
train_x <- data.matrix(select(xg_dat[train_ids,], -SalePrice))
train_y<-xg_dat[train_ids,]$SalePrice
test_x<-data.matrix(select(xg_dat[test_ids,], -SalePrice))
test_y<-xg_dat[test_ids,]$SalePrice
xgb_train<-xgb.DMatrix(data = train_x, label = train_y)
xgb_test<-xgb.DMatrix(data = test_x, label = test_y)
#Create model
xgbc<-xgboost(data = xgb_train, max.depth = 2, nrounds = 100) #We arbitrarily chose to run this for 100 rounds
#Note: it is hard to understand what exactly the train RMSE values here mean just because they are calculated based on the log-transformed SalePrice value.
pred_y = predict(xgbc, xgb_test)
sqrt(mean((exp(test_y) - exp(pred_y))^2))
library(e1071)
# Fitting Naive Bayes Model
set.seed(3)  # Setting Seed
classifier_cl <- naiveBayes(Neighborhood ~ ., dat[train_ids,])
#classifier_cl
# Predicting on test data'
y_pred <- predict(classifier_cl, newdata = dat[test_ids,])
# Confusion Matrix
cm <- table(dat[test_ids,]$Neighborhood, y_pred)
# Model Evaluation
mean(dat[test_ids,]$Neighborhood != y_pred)
cm
y_test
